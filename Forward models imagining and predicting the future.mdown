## Extending priors and loss functions


## Few-Shot Learning Through an Information Retrieval Lens. Triantafillou, 2017


## Deepmind Imagination-augmented agents

From the blog: The agents we introduce benefit from an ‘imagination encoder’- a neural network which learns to extract any information useful for the agent’s future decisions, but ignore that which is not relevant.
the imagination-augmented agents outperform the imagination-less baselines considerably: they learn with less experience and are able to deal with the imperfections in modelling the environment. Because agents are able to extract more knowledge from internal simulations they can solve tasks more with fewer imagination steps than conventional search methods, like the Monte Carlo tree search.
When we add an additional ‘manager’ component, which helps to construct a plan, the agent learns to solve tasks even more efficiently with fewer steps. In the spaceship task it can distinguish between situations where the gravitational pull of its environment is strong or weak, meaning different numbers of these imagination steps are required. When an agent is presented with multiple models of an environment, each varying in quality and cost-benefit, it learns to make a meaningful trade-off. Finally, if the computational cost of imagination increases with each action taken, the agent imagines the effect of multiple chained actions early, and relies on this plan later without invoking imagination again.





# Optimization as a model for few-shot learning. Ravi and Larochelle, 17





# Auxiliary tasks

For the auxiliary task litterature, exploring the idea that self-generating multiple tasks
can allow to learn efficiently single complex tasks, it is indeed a very interesting strand of work. However, the general idea is not new (what is new is to 
do it with DL algorithm), and for example in the team we have studied many dimensions which are not yet integrated into these DL architectures (and so
we have opportunities to make original contributions in DL by leveraging them). 

For example, instead of self-generating/sampling random goals/reward functions (like in Intentional Unintentional Agent), one can do this actively using 
bandits that maximize learning progress over goals. We are right now finishing a paper with Sébastien and Yoan to summarize some of these ideas in a 
formalism that is quite close to these DL papers, we will soon be able to circulate it.

If you are working on these issues, some classical papers of the team on them (outside DL yet) are:

Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots
Baranes, A., Oudeyer, P-Y. (2013)
Robotics and Autonomous Systems, 61(1), pp. 49-73.
http://www.pyoudeyer.com/ActiveGoalExploration-RAS-2013.pdf

Intrinsically Motivated Learning of Real-World Sensorimotor Skills with Developmental Constraints
Oudeyer P-Y., Baranes A., Kaplan F. (2013)
in Intrinsically Motivated Learning in Natural and Artificial Systems, eds. Baldassarre G. and Mirolli M., Springer
http://www.pyoudeyer.com/OudeyerBaranesKaplan13.pdf


## Physics states and property learning:

Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics
Ken Kansky Tom Silver David A. Mely Mohamed Eldawy Miguel L ´ azaro-Gredilla Xinghua Lou ´
Nimrod Dorfman Szymon Sidor Scott Phoenix Dileep George   
Schema Networks are closely related to Object-Oriented
MDPs (OO-MDPs) (Diuk et al., 2008) and Relational
MDPs (R-MDPs) (Guestrin et al., 2003a). 