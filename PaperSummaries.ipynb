{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MyID'></a>\n",
    "# Template\n",
    "*Authors* [pdf](wwwgooglefr)\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "### Goal\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "### Contribution\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "[Back_To_Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "<a id='Outline'></a>\n",
    "\n",
    "## Learning state\n",
    "\n",
    "- [Learning State Representations with Robotic Priors (2015)](#Jonschkowski15)\n",
    "- [PVEs : Position-Velocity Encoders for UnsupervisedLearning of Structured State Representations (2017)](#Jonschkowski17)\n",
    "- [Deep Spatial Autoencoders for Visuomotor Learning (2015)](#Finn15)\n",
    "- [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (2016)](#Chen16)\n",
    "\n",
    "## Learning next state\n",
    "\n",
    "- [Learning State Representation for Deep Actor-Critic Control (2016)](#Munk16)\n",
    "- [Stable reinforcement learning with autoencoders for tactile and visual data (2016)](#Hoof16)\n",
    "- [Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data (2017)](#Soelch17)\n",
    "- [Closing the learning-planning loop with predictive state representations (2009)](Boots09)\n",
    "\n",
    "\n",
    "## Learning to construct next image (or Observation) - >  Forward Model\n",
    "\n",
    "- [A physics-based model prior for object-oriented  MDPs (2014)](#Scholz14)\n",
    "- [Closing the learning-planning loop with predictive state representations (2009)](#Boots09)\n",
    "- [**[TODO]** Autonomous learning of state representations for control (2015)](#Bohmer15)\n",
    "\n",
    "\n",
    "## Learning to retrieve action\n",
    "\n",
    "\n",
    "\n",
    "## Learning a metric\n",
    "\n",
    "- [Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images (2015)](#Watter15)(#Jonschkowski17)\n",
    "\n",
    "## Applications\n",
    "## Validation Methods and Frameworks\n",
    "\n",
    "## Others\n",
    "\n",
    "- [Label-Free Supervision of Neural Networks with Physics and Domain Knowledge (2016)](#Stewart16)\n",
    "\n",
    "\n",
    "\n",
    "[To_The_End](#TheEND)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational models\n",
    "These models learn concepts as entities of the world and relational properties of theirs.\n",
    "\n",
    "Towards Deep Symbolic Reinforcement Learning, Garnelo et al. NIPS 2016.\n",
    "Handles three main components of neural-symbolic hybrid systems: 1)Conceptual abstraction. 2) Compositional structure. 3) Common sense priors, i.e., one of the first works bridging the gap among logics and neural models.\n",
    "\n",
    "Relational Networks (Santoro’17) and Visual Interaction Networks (Watters’17) are two philosophically similar models that use abstract logic to reason about the world. Relational reasoning is very closely linked to the elusive human \"common sense\", something that for a long time we thought not even other animals could do (eg \"what is the color of the object closest to the red square?\" \"How many objects have the same shape as the blue one?\"). Now this system achieves higher accuracy than humans.  [1706.01427] A simple neural network module for relational reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Feature Analysis : Unsupervised Learning of Invariances (2002)\n",
    "\n",
    "**\n",
    "\n",
    "\n",
    "This paper present the SFA methiod in order to find invariant features inside varying temporal signals.\n",
    "The approach is based on non linear expension of the signal with application of PCA.  The solution is gqrqnteed to find the optimal solution within a particular family of function. The feature exctracted by this method can be ordered by degree of invariance.\n",
    "Unfortunately the perfrormance decrease when the network ios trained to learn several features simultaneously but the result can be use in classification and recognition tasks.\n",
    "\n",
    "This paper present a first step into slow feature learning, it does not present scability to input dimension, the limitation and consequence of using SFA in hierqachical NN (even if an experience about it is done in the paper). The experement are not achieved with real images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Slow Feature Analysis\n",
    "\n",
    "*Varun Raj Kompella, Matthew Luciw, and Jurgen Schmidhuber*\n",
    "\n",
    "First use of the SFA framework into an online fashion thanks to a combination of PCA and MCA (Minor composant analysis). They also extend their work to hierarchical neurla networks an d use them for experiementing high dimensional video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A new embedding quality assement method for manifold learning\n",
    "\n",
    "Introduction of Normalization Independent Embedding Quality Assessment (NIEQA). Compared with current assessment methods which are limited to isometric embeddings, the NIEQA method has a much larger application range due to two features. \n",
    "- First,   it   is   based   on   a   new   measure   which   can   effectively evaluate  how  well  local  neighborhood  eometry  is  preserved under  normalization,  hence  it  can  be  applied  to  both  isometric and  normalized  embeddings.\n",
    "\n",
    "- Second,  it  can  provide  both  local and global evaluations to output an overall assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Stewart16'></a>\n",
    "\n",
    "# Label-Free Supervision of Neural Networks with Physics and Domain Knowledge (2016)\n",
    "*Russell Stewart , Stefano Ermon* [pdf](https://arxiv.org/pdf/1609.05566.pdf)\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "**Out of the scope** but related to prior learning because it use prior about physics.\n",
    "\n",
    "### Goal\n",
    "We are able to train a convolutional neural network to detect and track objects without any labeled examples\n",
    "\n",
    "\n",
    "### Method\n",
    "We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs.  These constraints are derived from prior domain knowledge, e.g., from known laws of physics.\n",
    "\n",
    "### Related Work\n",
    "- unsupervised deep learning to construct high level compressed embeddings of images without using label<br>\n",
    "**Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks** [pdf](http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvpr16-deepbit.pdf) <br>\n",
    "**Fast Training of Triplet-based Deep Binary Embedding Networks** [pdf](https://arxiv.org/pdf/1603.02844)<br>\n",
    "\n",
    "- The Deep Q-Network (DQN) provides another inspirational example for training neural networks with constraints rather than direct label<br>\n",
    "**Playing Atari with Deep Reinforcement Learning** [pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)<br>\n",
    "\n",
    "### Experiment\n",
    "- Tracking the position of a walking man\n",
    "- Detecting objects with causal relationships\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions. \n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning State Representations with Robotic Priors (2015)\n",
    "*Rico Jonschkowski Oliver Brock* [pdf](https://pdfs.semanticscholar.org/dc93/f6d1b704abf12bbbb296f4ec250467bcb882.pdf)\n",
    "\n",
    "<a id='Jonschkowski15'></a>\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- Learning a state\n",
    "- Learning with priors (physicxs andf robotics)\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Learning state representation with robotics priors to use the; with reinforcement learning afterwards\n",
    "\n",
    "### Method\n",
    "\n",
    "### Related Work\n",
    "- Share the term of prior with <br>\n",
    "** Representation learning: A review and new perspectives.** [pdf](link) <br>\n",
    "\n",
    "- Robotic task solved by RL <br>\n",
    "**Reinforcement  learning  in  robotics:  A  survey**\n",
    "\n",
    "- use of robotics priors to learn forward model (or transition function)\n",
    "**A physics-based model prior for object-oriented  MDPs**\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "We test how well our method can map 768-dimensional visual observations (16×16 pixels for red, green, and blue) into a two-dimensional or five-dimensional state space.\n",
    "\n",
    "### Conclusion\n",
    "The first key idea to this approach is to focus on state representation learning in the physical world instead of trying to solve the general problem of state representation learning in arbitrary (artificial) environment <br>\n",
    "We also show that the state representations learned by our method greatly improve generalization in reinforcement learning.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Scholz14'></a>\n",
    "# A physics-based model prior for object-oriented  MDPs (2014)\n",
    "*Jonathan Scholz, Martin Levihn, Charles L. Isbell, David Wingate*, [pdf](http://proceedings.mlr.press/v32/scholz14.pdf) [bib](http://dl.acm.org/citation.cfm?id=3045014)\n",
    "\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- Learning a forward model (next step)\n",
    "\n",
    "### Goal\n",
    "\n",
    "In  this paper we are primarily interested in the transition model T of a MDP ( Markov Decision Processes), and will consider several possible model priors P(T).\n",
    "Our results show that this representation can result in much faster learning,  by virtue of its strong but appropriate inductive bias in physical environments.\n",
    "\n",
    "### Method\n",
    "\n",
    "In this paper we presented two physics-inspired approaches to  modeling  object  dynamics  for  physical  domains.    The first,  OO-LWR,  leveraged  only  the  geometric  properties of  physical  dynamics,  and  the  second  extended  this  by exploiting modern physical simulation methods.   Our results suggest that PBRL has a learning bias which is well matched to RL tasks in physical domains.\n",
    "\n",
    "### Related Work\n",
    "- estimating physical parameters from data in robotics\n",
    "    - in vision\n",
    "**Model-based estimation of 3d human motion.** <br>\n",
    "**Motion estimation using physical simulatio** <br>\n",
    "    - in graphics for data-driven tuning of simulation parameter <br>\n",
    "**Computing the physical parameters of rigid-body motion from video** <br>\n",
    "**Estimating cloth simulation parameters from vide** <br>\n",
    "**Learning physics-based  motion  style  with  nonlinear  inverse  optimization.** <br>\n",
    "\n",
    "- controlling an initially unknown system and estimating its relevant parameters online <br>\n",
    "**Adaptive control: algorithms, analysis and applications** <br>\n",
    "*Adaptation  is  typically  done  in two  stages.   In  the  first  stage,  the  dynamical  system  parameters are estimated using a Parameter Adaptation Algorithm (PAA). In the next stage, these parameter estimates are used to update the controlle*\n",
    "### Experiment\n",
    "\n",
    "Our results show that PBRL is considerably more sample efficient than OO-LWR, potentially leading to qualitatively different behavior on large physical system.  OO-LWR, a generalization of OO-MDP that uses Locally-Weighted Regression as a core dynamics model. \n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The first,  OO-LWR,  leveraged  only  the  geometric  properties of  physical  dynamics,  and  the  second  extended  this  by exploiting modern physical simulation methods.   Our results suggest that PBRL ( Physics Based  Reinforcement Learning) has a learning bias which is well matched to RL tasks in physical domains.\n",
    "\n",
    "*********************************\n",
    "\n",
    "\n",
    "(from **Learning State Representations with Robotic Priors**) \n",
    "Instead of using a generic hypothesis space for the forward model, they use a restricted parametric hypothesis  space  based  on  physics.  This  can  be  very  helpful  for robotic tasks because we know that the right forward model must  lie  in  this  restricted  hypothesis  space.  However,  this work  assumes  to  already  have  a  suitable  state  representation  consisting  of  poses  and  velocities  as  well  as  knowledge  about  the  exact  semantics  of  every  state-dimension.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************\n",
    "\n",
    "*********************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Jonschkowski17'></a>\n",
    "# PVEs : Position-Velocity Encoders for UnsupervisedLearning of Structured State Representations (2017)\n",
    "*Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin Riedmiller* [pdf](https://arxiv.org/pdf/1705.09805.pdf)<br> \n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- Learning a state\n",
    "\n",
    "### Goal\n",
    "\n",
    "We propose position-velocity encoders (PVEs) which learn—without supervision—to encode images to positions  and velocities of task-relevant objects. \n",
    "\n",
    "### Method\n",
    "\n",
    "In  contrast  to  autoencoders,  position-velocity  encoders  are  not  trained  by  image  reconstruction, but by making the position-velocity representation consistent with priors about interacting with the physical world.\n",
    "\n",
    "### Related Work\n",
    "- Robotics Priors (physically grounded)<br>\n",
    "**Learning State Representations with Robotic Priors**<br>\n",
    "**A physics-based model prior for object-oriented  MDPs**<br>\n",
    "\n",
    "- use of restriction bias in visual representation \n",
    "    - In the Architecture :\n",
    "        - convolutional nework : **Convolutional  networks for images, speech, and time series**\n",
    "        - spatial transformer network : **Spatial transformer networks**\n",
    "        - spatial softmax : **End-to-end  training  of  deep  visuomotor  policies**\n",
    "        - end-to-end learnable histogram filters **End-to-end learnable  histogram  filters**\n",
    "        *incorporate  the  structure  of  the  Bayes’  filter  algorithm  for recursive  state  estimation*\n",
    "        - SE3-nets : **SE3-nets: Learning rigid body motion using deep neural networks**\n",
    "        *implement  assumptions about rigid body transformations*\n",
    "    - Preference biases expressed indirectly via other learnable functions based on the representation\n",
    "        - learn image reconstruction : **Embed to control: A locally linear latent dynamics model for control from raw images**\n",
    "        - prediction of future state : **Learning  to  filter  with  predictive  state inference  machines** AND **Embed to control: A locally linear latent dynamics model for control from raw images**\n",
    "        - other auxiliary task : **Reinforcement  learning  with unsupervised auxiliary tasks** AND ** Learning  to  navigate  incomplex  environments.**\n",
    "        -  learn symbol grounding : **Learning  grounded  relational  symbols  from  continuous data  for  abstract  reasoning**\n",
    "        - Direct preference biases are also the focus of metric learning, e.g. learning representations of faces using  the  fairly  general  triplet  loss **FaceNet:  A  unified  embedding  for  face  recognition and clustering.**\n",
    "\n",
    "### Experiment\n",
    "- Inverted Pendulum\n",
    "- Cart-Pole\n",
    "- Ball in Cup\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We applied PVEs to several simulated control tasks from pixels and  achieved  promising  preliminary  results.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Watter15'></a>\n",
    "# Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images (2015)\n",
    "*Manuel Watter Jost Tobias Springenberg Joschka Boedecker Martin Riedmiller* [pdf](https://pdfs.semanticscholar.org/21c9/dd68b908825e2830b206659ae6dd5c5bfc02.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    " - learning a state?\n",
    " - learning from control\n",
    "\n",
    "### Goal\n",
    "E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.\n",
    "\n",
    "### Method\n",
    "\n",
    "### Related Work\n",
    "- Survey  on representation learning for control  <br>\n",
    "**Autonomous learning of state representations for control** <br>\n",
    "- deep autoencoders (ignoring state transitions)<br>\n",
    "**Deep auto-encoder neural networks in reinforcement learning**<br>\n",
    "- control based on image streams <br>\n",
    "**Human-level control through deep reinforcement learning**\n",
    "- kernel based\n",
    "**Learning  of  non-parametric  control  policies  with  high-dimensional state features**\n",
    "- deep policy learning for robot control\n",
    "**End-to-end training of deep visuomotor policies.**\n",
    "- **From pixels to torques:  Policy learning with deep dynamical models**\n",
    "*Autoencoders are used to extract a latent representation for control from images, on which a non-linear model of the forward dynamics is learned. Their model is trained jointly and is thus similar to the non-linear E2C variant in our comparison. In contrast to our model, their formulation requires PCA pre-processing and does neither ensure that long-term predictions in latent space do not diverge, nor that they are linearizable.*\n",
    "- VAE family : <br>\n",
    "    - **Auto-encoding variational bayes**\n",
    "    - **Stochastic backpropagation and approximate inference in deep generative models.**\n",
    "    - ** DRAW: A recurrent neural network forimage generation.**\n",
    "    - **Learning stochastic recurrent networks.**\n",
    "- enforcing desired transformations in latent space during learning (such that the data becomes easy to model) <br>\n",
    "    - transforming auto-encoders : **Transforming auto-encoders.**\n",
    "    -  probabilistic models for images : **Nice: Non-linear independent components estimation.** AND **ransformation properties of learned visual representations.**\n",
    "-  learning relations between pairs of images : <br>\n",
    "    - **Dynamical binary latent variable models for 3d human pose trackin**\n",
    "    - **Learning to relate images**\n",
    "- state estimation in Markov decision processes\n",
    "    - for a discussion see : **Learning nonlinear dynamic models**\n",
    "    - **Bayesian Forecasting and Dynamic Models**\n",
    "    - **Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models**\n",
    "    \n",
    "\n",
    "### Experiment\n",
    "\n",
    "We evaluate our model on four visual tasks: an agent in a plane with obstacles, a visual version of the\n",
    "classic inverted pendulum swing-up task, balancing a cart-pole system, and control of a three-link\n",
    "arm with larger images.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We presented Embed to Control (E2C), a system for stochastic optimal control on high-dimensional image streams. Key to the approach is the extraction of a latent dynamics model which is constrained to be locally linear in its state transitions. An evaluation on four challenging benchmarks revealed that E2C can find embeddings on which control can be performed with ease, reaching performance close to that achievable by optimal control on the real system model\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Munk16'></a>\n",
    "# Learning State Representation for Deep Actor-Critic Control (2016)\n",
    "*Jelle Munk, Jens Kober and Robert Babuska* [pdf](http://www.jenskober.de/MunkCDC2016.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- predict next state\n",
    "- predict next reward\n",
    "\n",
    "### Goal\n",
    "\n",
    "In this paper, a  new  algorithm,  Model  Learning  Deep  Deterministic  Policy Gradient (ML-DDPG), is proposed that combines RL with state representation learning, i.e., learning a mapping from an input vector  to  a  state beforesolving  the  RL  task\n",
    "\n",
    "### Method\n",
    "\n",
    "The  ML-DDPG algorithm  uses  a  concept  we  call predictive priors to  learn  a model  network  which  is  subsequently  used  to  pre-train  the first  layer  of  the  actor  and  critic  networks.\n",
    "The  ML-DDPG  architecture  consist  of  three  DNNs,  a model network, a critic network and an actor network. The model network is trained by using a new concept that we call predictive priors and  is  integrated  with  the  actor  and  critic networks by copying some of its weights. \n",
    "\n",
    "the *predictable transition* prior which  states that,  given  a  certain  state $s_t$ and  an  action a t taken  in  that state,  one  can  predict  the  next  state $\\hat{s}_{t+1}$\n",
    "\n",
    "The second prior is the predictable reward prior which states that, given a  certain  state $s_t$ and  an  action $a_t$ taken  in  that  state,  onecan  predict  the  next  reward $\\hat{r}_{t+1}$. This  prior  enforces  that all  information  relevant  to  the  task  is  available  in  the  state, which  helps  the predictable transition prior to  converge  to a meaningful representation for the given task.\n",
    "\n",
    "### Related Work\n",
    "- uto-encoder is used to find an observation to state mapping  in  which  the  observations  are  compressed  into  a  low-dimensional  state  vector \n",
    "**Autonomous reinforcement learning on raw visual input data in a real world application,** <br>\n",
    "**Learning  visual  feature  spaces  for  robotic  manipulation  with  deep spatial autoencoders** <br>\n",
    "**Learning  deep dynamical models from image pixels** <br>\n",
    "- Slow feature analysis  used   to   learn   a   mapping   between   visual observations and a state representation that gradually changes over time\n",
    "**Reinforcement  learning on  slow  features  of  high-dimensional  input  streams** <br>\n",
    "**Learning  visual  feature  spaces  for  robotic  manipulation  with  deep spatial autoencoders** <br>\n",
    "- Robotics Prior (Jonschowsky)\n",
    "\n",
    "- predict $\\hat{o}_{t+1}$ instead of $\\hat{s}_{t+1}$\n",
    "**Efficient model learning methods for actor-critic control**\n",
    "**Learning grounded relational symbols from continuous data for abstract reasoning**\n",
    "\n",
    "### Experiment\n",
    "\n",
    "the 2-link arm problem : **The  importance of  experience  replay  database  composition  in  deep  reinforcement learning,**\n",
    "The octopus problem : **Learning  to  control  an octopus arm with Gaussian process temporal difference methods**\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Simulation  results show that the ML-DDPG can learn reasonable continuous control  policies  from  high-dimensional  observations  that  contain also  task-irrelevant  information.  Furthermore,  in  some  cases,this  approach  significantly  improves  the  final  performance  in comparison  to  end-to-end  learning.\n",
    "\n",
    "*********************************\n",
    "\n",
    "\n",
    "Use of the \"predictive prior to learn a task related state representation as a pretraining for the **ML-DDPG** algorithm (Model Learning Deterministic Policy Gradient).\n",
    "\n",
    "the prior aim to predict the next state and reward. It is supervised. :\n",
    "$L_m = ||  s_{t+1} - \\hat{s_{t+1}}  ||^2_2    +  \\lambda_m ||  r_{t+1} - \\hat{r_{t+1}}  ||^2_2 $\n",
    "\n",
    "\n",
    "it allows to train RL faster afterward. ( \"“End-to-end  training of deep visuomotor policies,\" claim it is not possible?)\n",
    "\n",
    "[Back To Outline](#Outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hoof16'></a>\n",
    "# Stable reinforcement learning with autoencoders for tactile and visual data (2016)\n",
    "*van Hoof, Herke, et al* [pdf](https://brml.org/uploads/tx_sibibtex/Hoof2016.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning state representation\n",
    "- learning next state\n",
    "\n",
    "\n",
    "### Goal\n",
    "we have introduced a reinforcement learning system consisting of two parts: an autoencoder that represents high-dimensional sensor representations in a compact space, and a reinforcement learner that is able to learn stable, non-linear policies. These stable updates allow on-policy learning with relatively small batches of data, making it possible to adapt the neural encoding during learning.\n",
    "\n",
    "### Method\n",
    "\n",
    "Auto-encoders specialize in finding compact representations, where defining such a metric is  likely  to  be  easier.  Therefore,  we  propose  a  reinforcement learning algorithm that can learn non-linear policies in contin- uous state spaces, which leverages representations learned using auto-encoders.\n",
    "\n",
    "### Related Work\n",
    "- Autoencoders [17]–[19] have shown to be very successful in learning meaningful low-dimensional representations of (robot) movement data [20]–[22]. Therefore, we propose us- ing the representation learned by such autoencoders as input for reinforcement learning of policies of non-task specific form\n",
    "-  One possibility is to use a neural network as a forward model for the dynamics and possibly the reward function. Planning or optimal control methods, such as model predictive control, can then be used to obtain optimal action. Recently, this approach was used to findoptimal policies when only simulated images of the system are given [23], [24]. ** The disadvantage of such methods is that gradients have to be propagated through a number of connections that is the product of the planning horizon and the network depth.**\n",
    "- Low-dimensional  representations  can  alternatively  be learned using autoencoders [17]–[19]. Such autoencoder are trained to reconstruct sensor inputs, as was done in [21]. owever, this approach does not ensure that the learned representations respects the task dynamics. For example, states that are easily reachable from the current state but have a different visual representation, might be far apart in the learned feature space . To address this issue, a smoothness penalty can be applied to the feature points, as was done in [22] to learn robot visuomotor tasks. More explicitly, the dynamics can be enforced to be linear in the latent space, as was done in the embed to control method [32].\n",
    "\n",
    "[17] **Extracting and composing robust features with denoising autoencoders**<br>\n",
    "[18] **Learning deep architectures for AI** <br>\n",
    "[19]**Auto-encoding variational Bayes**<br>\n",
    "[20] **Efficient movement representation by embedding dynamic movement primitives in deep autoencoders** <br>\n",
    "[21]**Learn to swing up and balance a real pole based on raw visual input data,**<br>\n",
    "[22]**Deep spatial autoencoders for visuomotor learning** <br>\n",
    "[23]**Data-efficient learning of feedback policies from image pixels using deep dynamical model**<br>\n",
    "[24] **From pixels to torques: Policy learning with deep dynamical model**<br>\n",
    "\n",
    "### Experiment\n",
    "\n",
    "In our experiments, first we compared different types of autoencoder on a simulated visual task. We found that better results were obtained for this task with variational autoencoders compared to the more traditional de-noising autoencoders. Modifying the objective to reproduce the sys- tem dynamics, rather than encode individual input patterns, also tended to improve reinforcement learning performance although the effect was not as pronounced. Re-training the encoders on the state distribution induced by the policy markedly improved performance. In this case, the encoder objective incurs the biggest loss for states that are most relevant to the policy, and therefore, such states are likely to be represented especially well in the learned feature space. In a second set-up, we considered a real-robot manip- ulation task based on tactile feedback. In this scenario, we showed that the robot was able to learn a policy that manipulates and stabilizes a platform. Rather then using joint encoders or handcrafted features, our algorithm learned the tasks based on complex and high dimensional tactile representations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For both tasks, we noticed that learning from the raw data tends to perform especially poorly in the presence of noise. Distances in the learned feature space are not distorted by noise as much as those in the image space, as the learned feature representation tends to average over multiple input channels. Thus, reinforcement learning on the learned feature space is still successful for the noisy task. We consider this real-robot task to be a first step towards more complex manipulation tasks, where tactile feedback has the potential to improve robustness with respect to perturbances and inaccuracies. We are currently working on learning manipulation policies on multi-fingered robotic hands. In future work, we plan to investigate efficient explo- ration strategies that are critical for success in this domain.\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Finn15'></a>\n",
    "# Deep Spatial Autoencoders for Visuomotor Learning (2015)\n",
    "*Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel* [pdf](https://arxiv.org/abs/1509.06113)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning a state\n",
    "\n",
    "### Goal\n",
    "\n",
    "We present an approach that  automates  state-space  construction  by  learning  a  state representation  directly  from  camera  images. \n",
    "\n",
    "### Method\n",
    "\n",
    "Our  method  uses\n",
    "a  deep  spatial  autoencoder  to  acquire  a  set  of  feature  points that  describe  the  environment  for  the  current  task,  such  as the  positions  of  objects,  and  then  learns  a  motion  skill  with these  feature  points  using  an  efficient  reinforcement  learning method  based  on  local  linear  models.\n",
    "\n",
    "### Related Work\n",
    "Note : this related work is cool! it gives clear view about the state of the art and relation between things.\n",
    "\n",
    "- Priors<br>\n",
    "    - Jonschowsky <br>\n",
    "- ability  to predict  future  observations \n",
    "    - **Closing  the  learning-planning loop with predictive state representations**\n",
    "    - **Autonomous  learning  of  state  representations  for control** \n",
    "    - **Learning predictive state representations** \n",
    "- autoencoders : **Autonomous reinforcement learning on raw visual input data in a real world application**\n",
    "- train autoencoders that can predict the next  image : **E2C** AND **Learning to linearize under uncertainty**\n",
    "\n",
    "-  The quantity of data required to apply deep learning methods to real-world images is known to far exceed  that  of  toy  tasks  **Deep learning - Nature**\n",
    "\n",
    "- Other methods have been proposed to train policies from high-dimensional observations, without first acquiring a low-dimensional  state  space  (**Playing  Atari  with  deep  reinforcement learning**),  (**End-to-end training of deep visuomotor policies**).  However,  running  these methods on a real robotic platform requires either impractical amounts  of  data  [22]  or  an  instrumented  training  setup that  provides  knowledge  about  relevant  objects  at  training time  (**End-to-end training of deep visuomotor policies**).\n",
    "\n",
    "- The goal of our method is similar to that of visual servoing, which performs feedback control on image features (**A  new  approach  to  visual servoing in robotics**),(**Vision  based  control  of  a quadrotor for perching on planes and lines,**),  (**elative  end-effector control using cartesian position based visual servoing**) However,  our  controllers  and  visual  features  are learned entirely from real-world data, rather than being hand-specified.\n",
    "\n",
    "-  Furthermore, our approach does not require any sort of camera calibration, in contrast to many visual servoing methods (though not all – see e.g. [27], [28]).\n",
    "\n",
    "### Experiment\n",
    "\n",
    "The  resulting  controller reacts  continuously  to  the  learned  feature  points,  allowing  the robot  to  dynamically  manipulate  objects  in  the  world  withclosed-loop  control.  We  demonstrate  our  method  with  a  PR2 robot  on  tasks  that  include  pushing  a  free-standing  toy  block, picking  up  a  bag  of  rice  using  a  spatula,  and  hanging  a  loopof  rope  on  a  hook  at  various  positions.  \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In  each  task,  our method automatically learns to track task-relevant objects andmanipulate  their  configuration  with  the  robot’s  arm.\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Soelch17'></a>\n",
    "# Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data (2017)\n",
    "*Maximilian Soelch, Justin Bayer, Patrick van der Smagt* [pdf](https://openreview.net/pdf?id=HyTqHL5xg)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning a next state\n",
    "\n",
    "### Goal\n",
    "\n",
    "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models\n",
    "\n",
    "### Method\n",
    "\n",
    "Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference.  Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge.\n",
    "\n",
    "### Contribution\n",
    "\n",
    "Our contribution is, to our knowledge, the first model that (i) enforces the latent state-space model assumptions, allowing for reliable system identification, and plausible long-term prediction of the observable system, (ii) provides the corresponding inference mechanism with rich dependencies,(iii) inherits the merit of neural architectures to be trainable on raw data such as images or other sensory inputs, and (iv) scales to large data due to optimization of parameters based on stochastic gradient descent\n",
    "\n",
    "### Related Work\n",
    "\n",
    "\n",
    "- Estimating probabilistic models for sequential data is central to many domains, such as audio, natural language or physical plants, Graves (2013) (**Generating sequences with recurrent neural networks**); Watter et al. (2015); (**E2C**) Chung et al. (2015) **A recurrent latent variable model for sequential data.**; Deisenroth &Rasmussen (2011) **pilco: A model-based and data-efficient approach to policy search**; Ko & Fox (2011). The goal is to obtain a model p(x1:T) that best reflects a dataset of observed sequences x1:T. Recent advances in deep learning have paved the way to powerfulmodels capable of representing high-dimensional sequences with temporal dependencies, e.g., Graves(2013); Watter et al. (2015); Chung et al. (2015); Bayer & Osendorfer (2014\n",
    "\n",
    "- Leveraging a recently proposed estimator based on variational inference, stochastic gradient variational Bayes (SGVB, Kingma & Welling (2013); Rezende et al. (2014)), approximate inference of latent variables becomes tractable. Extensions to time series have been shown in Bayer & Osendorfer(2014); Chung et al. (2015). Empirically, they showed considerable improvements in marginal data likelihood, i.e., compression, but lack full-information latent states, which prohibits, e.g., long-term sampling. Yet, in a wide range of applications, full-information latent states should be valued over compression. This is crucial if the latent spaces are used in downstream applications\n",
    "\n",
    "- Time series for dynamic systems have been studied extensively in systems theory, cf. McGoff et al. (2015) and sources therein.  In particular, state space models have shown to be a powerful tool to  analyze and control the dynamics.  Two tasks remain a significant challenge to this day:  Can we identify the governing system from data only? And can we perform inference from observables to the latent system variables? These two tasks are competing: A more powerful representation of system requires more computationally demanding inference, and efficient inference, such as the well-known Kalman filters, Kalman & Bucy (1961), can prohibit sufficiently complex system classes.\n",
    "\n",
    "- non linear kallman filter : Julier & Uhlmann (1997)\n",
    "*are successfully applied in many areas, they suffer from two major drawbacks: firstly, its assumptions are restrictive and are violated in practical applications, leading toasuboptimal results. Secondly, parameters such as t and Bt have to be known in order to perform posterior inference.*\n",
    "\n",
    "-  There have been efforts to learn such system dynamics, cf. Ghahramani & Hinton (1996); Honkela et al. (2010) based on the expectation maximization (EM) algorithm or Valpola & Karhunen (2002), which uses neural networks. However, these algorithms are not applicable in cases where the true posterior distribution is intractable. This is the case if, e.g., image sequences are used, since the posterior is then highly nonlinear—typical mean-field assumptions on the approximate posterior are too simplified.  Our new approach will tackle both issues, and moreover learn both identification and inference jointly by exploiting Stochastic Gradient Variational Bayes.\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "- dynamic Pendulum\n",
    "- bouncing ball (and two bouncing balls !)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction\n",
    "\n",
    " In a series of vision-based experiments we demonstrated that latent states can be recovered which identify the underlying physical quantities. The generative model showed stable long-term predictions far beyond the sequence length used during training.\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Goroshin15'></a>\n",
    "# Learning to linearize under uncertainty (2015)\n",
    "*R. Goroshin, M. Mathieu, and Y. LeCun* [pdf](https://arxiv.org/pdf/1506.03011.pdf)\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- predict fuiture observation\n",
    "\n",
    "### Goal\n",
    "Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However,a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences\n",
    "\n",
    "### Method\n",
    "\n",
    "This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "- learning features from video using varying degrees of supervision / works have also trained deep networks for the task of frame prediction **12** **13** **14**  However, unlike other works that focus on prediction as a final objective, in this work prediction is regarded as a proxy for learning representations\n",
    "\n",
    "- Several recent works propose schemes for learning representations from video which use varying degrees of supervision **12** **14** **13** **4**. For instance, **13** assumes that the pre-trained network from **7**  is  already  available  and  training  consists  of  learning  to  mimic  this  network.   Similarly,  **14** learns a representation by receiving supervision from a tracker. This work is more closely related to fully unsupervised approaches for learning representations from video such as **4** **6** **2** **15** **8**.  It is most related to **12** which also trains a decoder to explicitly predict video frames. Our proposed architecture was inspired by those presented in in **11** and **Deconvolutional network**.\n",
    "\n",
    "- use of \"capsule\" unit : Hinton et al. **5**\n",
    "*In that work,  an equivariant representation is learned by the capsules when the true latent states were provided to the network as implicit targets.   Our work allows us to move to a more unsupervised setting in which the true latent states are not only unknown, but represent  completely arbitrary qualities.  This was made possible with two assumptions:  (1) that temporallyadjacent samples also correspond to neighbors in the latent space, (2) predictions of future samples  can be formulated as linear operations in the latent space.   In theory,  the representation learned by our method is very similar to the representation learned by the “capsules”;  this representation has a locally stable “what” component and a locally linear,  or equivariant “where” component*\n",
    "\n",
    "**4** : Unsupervised learning of spatio temporally coherent metrics <br>\n",
    "**5** : Transforming auto-encoders *Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang* <br>\n",
    "**6** : Extracting slow subspaces from natural videos leads to complex cells <br>\n",
    "**7** : Imagenet classification with deep convolutional neural network <br>\n",
    "**8** : Deep learning from temporal coherence in video <br>\n",
    "**11** : Unsupervised learning of invariant feature hierarchies with applications to object recognition <br>\n",
    "**12** : Video (language) modeling: a baseline for generative models of natural videos <br>\n",
    "**13** : Anticipating the future by watching unlabeled video <br>\n",
    "**14** : Unsupervised learning of visual representations using videos <br>\n",
    "**15** : Slow feature analysis: Unsupervised learning of invariances <br>\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "In the first set of experiments we train a shallow architecture on natural data and visualize the learned featuresin order gain a basic intuition.   In the second set of experiments we train a deep architecture on simulated movies generated from the NORB dataset.  By generating frames from interpolated and extrapolated points in code space we show that a linearized representation of the input is learned. Finally, we explore the role of uncertainty by training on only partially predictable sequences, we show that our latent variable formulation can account for this uncertainty enabling the encoder to learn a linearized representation even in this setting.\n",
    "\n",
    "### Conclusion\n",
    "In  this  work  we  have  proposed  a  new  loss  and  architecture  for  learning  locally  linearized  features from video.  We have also proposed a method that introduces latent variables that are non-deterministic functions of the input for coping with inherent uncertainty in video.  In future work we will suggest methods for “stacking” these architectures that will linearize more complex features over longer temporal scales.\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Boots09'></a>\n",
    "\n",
    "# Closing the learning-planning loop with predictive state representations (2009)\n",
    "*Byron Boots, Sajid M. Siddiqi, Geoffrey J. Gordon* [pdf](http://www.cs.cmu.edu/~ggordon/boots-siddiqi-gordon-closing-loop-psrs.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning to predict next observation\n",
    "\n",
    "### Goal\n",
    "\n",
    "we  propose  a novel algorithm which provably learns a compact, accurate model directly  from  sequences  of  action-observation  pairs. \n",
    "\n",
    "### Contribution\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "To  evaluate the learner, we then close the loop from observations to actions: we plan in the learned model and recover a policy which is near- optimal  in  the original environment  (not  the  model).  In  more detail, we present a spectral algorithm for learning a Predictive State  Representation  (PSR).   Our algorithm has several benefits which have not appeared together in  any  previous  PSR  learner:  it  is  computationally  efficient  and statistically  consistent;  it  handles  high-dimensional  observations and long time horizons by working from real-valued features of observation sequences; and finally, our close-the-loop experiments provide  an  end-to-end  practical  test\n",
    "\n",
    "### Related Work\n",
    "\n",
    "- Note : One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.\n",
    "\n",
    "-  For example, Expectation-Maximization (EM) [1] does not avoid local min-ima or scale to large state spaces; and, although many learning algorithms  have  been  proposed  for  PSRs  [22,  31,  14,  27,  2] and OOMs [8, 5, 12], none have been shown to learn models that are accurate enough for planning\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "We  demonstrate  the  algorithm  by learning  a  model  of  a  simulated  high-dimensional,  vision-based mobile  robot  planning  task,  and  then  performing  approximate point-based  planning  in  the  learned  model.  This  experiment shows that the learned PSR captures the essential features of the environment, allows accurate prediction with a small number of parameters,  and  enables  successful  and  efficient  planning.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We  have  presented  a  novel consistent subspace  identifica- tion  algorithm  that  simultaneously  solves  the discovery and learning problems  for  TPSRs.  In  addition,  we  provided  two extensions to the learning algorithm that are useful in practice, while  maintaining  consistency:  characteristic  and  indicative features only require one to know relevant features of tests and histories,  rather  than  core  sets  of  tests  and  sufficient  sets  of histories, while kernel density estimation can be used to find observable  operators  when  observations  are  real-valued.  We also showed how point-based approximate planning techniques can  be  used  to  solve  the planning problem  in  the  learned model.  We  demonstrated  the  representational  capacity  of  our model and the effectiveness of our learning algorithm by learn- ing a compact model from simulated autonomous robot vision data. Finally, we closed the loop by successfully planning with the learned models. To our knowledge this is the first instance of learning a model for a simulated robot in a nonlinear, non- Gaussian, partially observable environment of this complexity using a consistent algorithm and successfully planning in the learned model. We compare the policy generated by our model to a bound on the best possible value, and determine that our policy is close to optimal. We  believe  the  spectral  PSR  learning  algorithm  presented here,   and   subspace   identification   procedures   for   learning PSRs  in  general,  can  increase  the  scope  of  planning  under uncertainty  for  autonomous  agents  in  previously  intractable scenarios.  We  believe  that  this  improvement  is  partly  due to  the  greater  representational  power  of  PSRs  as  compared to  POMDPs,  and  partly  due  to  the  efficient  and  statistically consistent nature of the learning method.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Bohmer15'></a>\n",
    "# Autonomous learning of state representations for control (2015)\n",
    "*Wendelin Bohmer, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller, Klaus Obermayer* [pdf](http://www.ni.tu-berlin.de/fileadmin/fg215/articles/boehmer15b.pdf)\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning a forward model (see at [Finn15](#Finn15) )\n",
    "\n",
    "### Goal\n",
    "\n",
    "we review two approaches to learn intermediate state  representations from  previous  experiences: deep  auto-encoders and slow-feature  analysis.\n",
    "\n",
    "### Method\n",
    "\n",
    "We  analyze theoretical properties of the representations and point to potential improvements.\n",
    "\n",
    "### Contribution\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "\n",
    "### Key Statements\n",
    "\n",
    "A state that a good state representation should be :\n",
    "- Markovian\n",
    "- Must  be  able  to represent  the  true  value of  the current policy well enough for policy improvement\n",
    "- must generalize the learned value-function to unseen states with similar futures\n",
    "- must be low dimensional for efficient estimation-\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "[Back_To_Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Krishnan15'></a>\n",
    "# Deep Kalman Filters (2015)\n",
    "*Rahul G. Krishnan, Uri Shalit, David Sontag* [pdf](https://arxiv.org/pdf/1511.05121)\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning a state\n",
    "\n",
    "### Goal\n",
    "\n",
    "Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines.  Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters.  Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the “Healing MNIST” dataset where long-term structure, noise and actions are applied to sequences of digits\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "Our goal is to fit a generative model to a sequence of observations and actions, motivated by the nature of patient health record data. We assume that the observations come from a latent state which evolves over time.  We assume the observations are a noisy, non-linear function of this latent state. Finally,  we  also  assume  that  we  can  observe  actions,  which  affect  the  latent  state  in  a  possibly non-linear manner.\n",
    "\n",
    "### Contribution\n",
    "\n",
    "- Develop a method for probabilistic generative modelling of sequences of complex observations, perturbed by non-linear actions, using deep neural nets as a building block.  We derive a bound on the log-likelihood of sequential data and an algorithm to learn a broad class of Kalman filters.\n",
    "- We evaluate the efficacy of different recognition distributions for inference and learning.\n",
    "- We consider this model for use in counterfactual inference with emphasis on the medical setting.  To the best of our knowledge, the use of continuous state space models has not been considered for this goal. On a synthetic setting we empirically validate that our model is able to capture patterns within a very noisy setting and model the effect of external actions. On real patient data we show that our model can successfully perform counterfactualinference to show the effect of anti-diabetic drugs on diabetic patients\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    " We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We show promising results that nonlinear-state space models can be effective models for counter-factual analysis.   The parametric posterior can be used to approximate the latent state of unseen  data.  We can forward sample from the model under different actions and observe their consequenteffect.  Beyond counterfactual inference, the model represents a natural way to embed patients into latent space making it possible to ask questions about patient similarity. Another avenue of work is understanding whether the latent variable space encodes identifiable characteristics of a patient and whether the evolution of the latent space corresponds to known disease trajectories\n",
    "\n",
    "\n",
    "[Back_To_Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TheEND'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Chen16'></a>\n",
    "# InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\n",
    "*Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel* [pdf](https://arxiv.org/abs/1606.03657)\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning a state\n",
    "\n",
    "### Goal\n",
    "\n",
    "This paper describes InfoGAN, an information-theoretic extension to the Gener-\n",
    "ative Adversarial Network that is able to learn disentangled representations in a\n",
    "completely unsupervised manner.\n",
    "\n",
    "### Method\n",
    "\n",
    "InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently\n",
    "\n",
    "### Contribution\n",
    "\n",
    "### Related Work\n",
    "- Unsupervised representation learning<br>\n",
    "    - stacked (often denoising) autoencoders or restricted Boltzmann machines [10–13]\n",
    "    - skip-thought vectors [15] \n",
    "    - unsupervised feature learning of images [16]\n",
    "- adder network [17], which has achieved spectacular results on a semi-supervised variant of the MNIST dataset.\n",
    "- VAE has achieved even better semi-supervised results on MNIST [18]\n",
    "- Lake et al. [20] have been able to learn representations using probabilistic inference over Bayesian programs\n",
    "- GAN - DCGAN\n",
    "- learn disentangled representation using supervised data\n",
    "    - One class of such methods trains a subset of the representation to match the supplied label using supervised learning:  bilinear models [21] separate style and content; multi-view perceptron [22] separate face identity and view point; and Yanget al. [23] developed a recurrent variant that generates a sequence of latent factor transformations. Similarly, VAEs [5] and Adversarial Autoencoders [9] were shown to learn representations in which class label is separated from other variations.\n",
    "- weakly  supervised  methods  were  developed  to  remove  the  need  of  explicitly labeling variations. disBM [24] is a higher-order Boltzmann machine which learns a disentangled representation by “clamping” a part of the hidden units for a pair of data points that are known to match in all but one factors of variation\n",
    "-   DC-IGN [7] extends this “clamping” idea to VAE and successfully learns graphics codes that can represent pose and light in 3D rendered images \n",
    "- Whitney et al. [8] proposed to alleviate the grouping requirement by learning from consecutive frames of images and use temporal continuity as supervisory signal.\n",
    "- To the best of our knowledge, the only other unsupervised method that learns disentangled representations is hossRBM [13],  a higher-order extension of the spike-and-slab restricted Boltzmann machine that can disentangle emotion from identity on the Toronto Face Dataset However, hossRBM can only disentangle discrete latent factors, and its computation cost grows exponentially in the number of factors. InfoGAN can disentangle both discrete and continuous latent factors, scale to complicated datasets, and typically requires no more training time than regular GAN.\n",
    "\n",
    "### Experiment\n",
    "\n",
    "Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, pres- ence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This paper introduces a representation learning algorithm called Information Maximizing Generative Adversarial Networks (InfoGAN). In contrast to previous approaches, which require supervision, InfoGAN is completely unsupervised and learns interpretable and disentangled representations In addition, InfoGAN adds only negligible computation cost on top of GAN and is easy to train.\n",
    "\n",
    "[Back_To_Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO or Investigate \n",
    "\n",
    "- **Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks** (from [Stewart16](#Stewart16) )\n",
    "- **Fast Training of Triplet-based Deep Binary Embedding Networks**  (from [Stewart16](#Stewart16) )\n",
    "- **Learn to swing up and balance a real pole based on raw visual input data** (from [Hoof15](#Hoof15) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
