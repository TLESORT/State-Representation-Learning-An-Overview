{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template\n",
    "*Authors* [pdf](www.google>fr)\n",
    "\n",
    "<a id='MyID'></a>\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "### Goal\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "<a id='Outline'></a>\n",
    "\n",
    "- [Label-Free Supervision of Neural Networks with Physics and Domain Knowledge (2016)](#Stewart16)\n",
    "- [Learning State Representations with Robotic Priors (2015)](#Jonschkowski15)\n",
    "- [A physics-based model prior for object-oriented  MDPs (2014)](#Scholz14)\n",
    "- [PVEs : Position-Velocity Encoders for UnsupervisedLearning of Structured State Representations (2017)](#Jonschkowski17)\n",
    "- [Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images (2015)](#Watter15)\n",
    "- [Learning State Representation for Deep Actor-Critic Control (2016)](#Munk16)\n",
    "- [Stable reinforcement learning with autoencoders for tactile and visual data (2016)](#Hoof16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational models\n",
    "These models learn concepts as entities of the world and relational properties of theirs.\n",
    "\n",
    "Towards Deep Symbolic Reinforcement Learning, Garnelo et al. NIPS 2016.\n",
    "Handles three main components of neural-symbolic hybrid systems: 1)Conceptual abstraction. 2) Compositional structure. 3) Common sense priors, i.e., one of the first works bridging the gap among logics and neural models.\n",
    "\n",
    "Relational Networks (Santoro’17) and Visual Interaction Networks (Watters’17) are two philosophically similar models that use abstract logic to reason about the world. Relational reasoning is very closely linked to the elusive human \"common sense\", something that for a long time we thought not even other animals could do (eg \"what is the color of the object closest to the red square?\" \"How many objects have the same shape as the blue one?\"). Now this system achieves higher accuracy than humans.  [1706.01427] A simple neural network module for relational reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow Feature Analysis : Unsupervised Learning of Invariances (2002)\n",
    "\n",
    "**\n",
    "\n",
    "\n",
    "This paper present the SFA methiod in order to find invariant features inside varying temporal signals.\n",
    "The approach is based on non linear expension of the signal with application of PCA.  The solution is gqrqnteed to find the optimal solution within a particular family of function. The feature exctracted by this method can be ordered by degree of invariance.\n",
    "Unfortunately the perfrormance decrease when the network ios trained to learn several features simultaneously but the result can be use in classification and recognition tasks.\n",
    "\n",
    "This paper present a first step into slow feature learning, it does not present scability to input dimension, the limitation and consequence of using SFA in hierqachical NN (even if an experience about it is done in the paper). The experement are not achieved with real images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Slow Feature Analysis\n",
    "\n",
    "*Varun Raj Kompella, Matthew Luciw, and Jurgen Schmidhuber*\n",
    "\n",
    "First use of the SFA framework into an online fashion thanks to a combination of PCA and MCA (Minor composant analysis). They also extend their work to hierarchical neurla networks an d use them for experiementing high dimensional video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A new embedding quality assement method for manifold learning\n",
    "\n",
    "Introduction of Normalization Independent Embedding Quality Assessment (NIEQA). Compared with current assessment methods which are limited to isometric embeddings, the NIEQA method has a much larger application range due to two features. \n",
    "- First,   it   is   based   on   a   new   measure   which   can   effectively evaluate  how  well  local  neighborhood  eometry  is  preserved under  normalization,  hence  it  can  be  applied  to  both  isometric and  normalized  embeddings.\n",
    "\n",
    "- Second,  it  can  provide  both  local and global evaluations to output an overall assessment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Label-Free Supervision of Neural Networks with Physics and Domain Knowledge (2016)\n",
    "*Russell Stewart , Stefano Ermon* [pdf](https://arxiv.org/pdf/1609.05566.pdf)\n",
    "\n",
    "<a id='Stewart16'></a>\n",
    "\n",
    "### Classification for overview\n",
    "**Out of the scope** but related to prior learning because it use prior about physics.\n",
    "\n",
    "### Goal\n",
    "We are able to train a convolutional neural network to detect and track objects without any labeled examples\n",
    "\n",
    "\n",
    "### Method\n",
    "We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs.  These constraints are derived from prior domain knowledge, e.g., from known laws of physics.\n",
    "\n",
    "### Related Work\n",
    "- unsupervised deep learning to construct high level compressed embeddings of images without using label<br>\n",
    "**Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks** [pdf](http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvpr16-deepbit.pdf) <br>\n",
    "**Fast Training of Triplet-based Deep Binary Embedding Networks** [pdf](https://arxiv.org/pdf/1603.02844)<br>\n",
    "\n",
    "- The Deep Q-Network (DQN) provides another inspirational example for training neural networks with constraints rather than direct label<br>\n",
    "**Playing Atari with Deep Reinforcement Learning** [pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)<br>\n",
    "\n",
    "### Experiment\n",
    "- Tracking the position of a walking man\n",
    "- Detecting objects with causal relationships\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions. \n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning State Representations with Robotic Priors (2015)\n",
    "*Rico Jonschkowski Oliver Brock* [pdf](https://pdfs.semanticscholar.org/dc93/f6d1b704abf12bbbb296f4ec250467bcb882.pdf)\n",
    "\n",
    "<a id='Jonschkowski15'></a>\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- Learning a state\n",
    "- Learning with priors (physicxs andf robotics)\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Learning state representation with robotics priors to use the; with reinforcement learning afterwards\n",
    "\n",
    "### Method\n",
    "\n",
    "### Related Work\n",
    "- Share the term of prior with <br>\n",
    "** Representation learning: A review and new perspectives.** [pdf](link) <br>\n",
    "\n",
    "- Robotic task solved by RL <br>\n",
    "**Reinforcement  learning  in  robotics:  A  survey**\n",
    "\n",
    "- use of robotics priors to learn forward model (or transition function)\n",
    "**A physics-based model prior for object-oriented  MDPs**\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "We test how well our method can map 768-dimensional visual observations (16×16 pixels for red, green, and blue) into a two-dimensional or five-dimensional state space.\n",
    "\n",
    "### Conclusion\n",
    "The first key idea to this approach is to focus on state representation learning in the physical world instead of trying to solve the general problem of state representation learning in arbitrary (artificial) environment <br>\n",
    "We also show that the state representations learned by our method greatly improve generalization in reinforcement learning.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Scholz14'></a>\n",
    "# A physics-based model prior for object-oriented  MDPs (2014)\n",
    "*Jonathan Scholz, Martin Levihn, Charles L. Isbell, David Wingate*, [pdf](http://proceedings.mlr.press/v32/scholz14.pdf) [bib](http://dl.acm.org/citation.cfm?id=3045014)\n",
    "\n",
    "\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- Learning a forward model (next step)\n",
    "\n",
    "### Goal\n",
    "\n",
    "In  this paper we are primarily interested in the transition model T of a MDP ( Markov Decision Processes), and will consider several possible model priors P(T).\n",
    "Our results show that this representation can result in much faster learning,  by virtue of its strong but appropriate inductive bias in physical environments.\n",
    "\n",
    "### Method\n",
    "\n",
    "In this paper we presented two physics-inspired approaches to  modeling  object  dynamics  for  physical  domains.    The first,  OO-LWR,  leveraged  only  the  geometric  properties of  physical  dynamics,  and  the  second  extended  this  by exploiting modern physical simulation methods.   Our results suggest that PBRL has a learning bias which is well matched to RL tasks in physical domains.\n",
    "\n",
    "### Related Work\n",
    "- estimating physical parameters from data in robotics\n",
    "    - in vision\n",
    "**Model-based estimation of 3d human motion.** <br>\n",
    "**Motion estimation using physical simulatio** <br>\n",
    "    - in graphics for data-driven tuning of simulation parameter <br>\n",
    "**Computing the physical parameters of rigid-body motion from video** <br>\n",
    "**Estimating cloth simulation parameters from vide** <br>\n",
    "**Learning physics-based  motion  style  with  nonlinear  inverse  optimization.** <br>\n",
    "\n",
    "- controlling an initially unknown system and estimating its relevant parameters online <br>\n",
    "**Adaptive control: algorithms, analysis and applications** <br>\n",
    "*Adaptation  is  typically  done  in two  stages.   In  the  first  stage,  the  dynamical  system  parameters are estimated using a Parameter Adaptation Algorithm (PAA). In the next stage, these parameter estimates are used to update the controlle*\n",
    "### Experiment\n",
    "\n",
    "Our results show that PBRL is considerably more sample efficient than OO-LWR, potentially leading to qualitatively different behavior on large physical system.  OO-LWR, a generalization of OO-MDP that uses Locally-Weighted Regression as a core dynamics model. \n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The first,  OO-LWR,  leveraged  only  the  geometric  properties of  physical  dynamics,  and  the  second  extended  this  by exploiting modern physical simulation methods.   Our results suggest that PBRL ( Physics Based  Reinforcement Learning) has a learning bias which is well matched to RL tasks in physical domains.\n",
    "\n",
    "*********************************\n",
    "\n",
    "\n",
    "(from **Learning State Representations with Robotic Priors**) \n",
    "Instead of using a generic hypothesis space for the forward model, they use a restricted parametric hypothesis  space  based  on  physics.  This  can  be  very  helpful  for robotic tasks because we know that the right forward model must  lie  in  this  restricted  hypothesis  space.  However,  this work  assumes  to  already  have  a  suitable  state  representation  consisting  of  poses  and  velocities  as  well  as  knowledge  about  the  exact  semantics  of  every  state-dimension.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************\n",
    "\n",
    "*********************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Jonschkowski17'></a>\n",
    "# PVEs : Position-Velocity Encoders for UnsupervisedLearning of Structured State Representations (2017)\n",
    "*Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin Riedmiller* [pdf](https://arxiv.org/pdf/1705.09805.pdf)<br> \n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- Learning a state\n",
    "\n",
    "### Goal\n",
    "\n",
    "We propose position-velocity encoders (PVEs) which learn—without supervision—to encode images to positions  and velocities of task-relevant objects. \n",
    "\n",
    "### Method\n",
    "\n",
    "In  contrast  to  autoencoders,  position-velocity  encoders  are  not  trained  by  image  reconstruction, but by making the position-velocity representation consistent with priors about interacting with the physical world.\n",
    "\n",
    "### Related Work\n",
    "- Robotics Priors (physically grounded)<br>\n",
    "**Learning State Representations with Robotic Priors**<br>\n",
    "**A physics-based model prior for object-oriented  MDPs**<br>\n",
    "\n",
    "- use of restriction bias in visual representation \n",
    "    - In the Architecture :\n",
    "        - convolutional nework : **Convolutional  networks for images, speech, and time series**\n",
    "        - spatial transformer network : **Spatial transformer networks**\n",
    "        - spatial softmax : **End-to-end  training  of  deep  visuomotor  policies**\n",
    "        - end-to-end learnable histogram filters **End-to-end learnable  histogram  filters**\n",
    "        *incorporate  the  structure  of  the  Bayes’  filter  algorithm  for recursive  state  estimation*\n",
    "        - SE3-nets : **SE3-nets: Learning rigid body motion using deep neural networks**\n",
    "        *implement  assumptions about rigid body transformations*\n",
    "    - Preference biases expressed indirectly via other learnable functions based on the representation\n",
    "        - learn image reconstruction : **Embed to control: A locally linear latent dynamics model for control from raw images**\n",
    "        - prediction of future state : **Learning  to  filter  with  predictive  state inference  machines** AND **Embed to control: A locally linear latent dynamics model for control from raw images**\n",
    "        - other auxiliary task : **Reinforcement  learning  with unsupervised auxiliary tasks** AND ** Learning  to  navigate  incomplex  environments.**\n",
    "        -  learn symbol grounding : **Learning  grounded  relational  symbols  from  continuous data  for  abstract  reasoning**\n",
    "        - Direct preference biases are also the focus of metric learning, e.g. learning representations of faces using  the  fairly  general  triplet  loss **FaceNet:  A  unified  embedding  for  face  recognition and clustering.**\n",
    "\n",
    "### Experiment\n",
    "- Inverted Pendulum\n",
    "- Cart-Pole\n",
    "- Ball in Cup\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We applied PVEs to several simulated control tasks from pixels and  achieved  promising  preliminary  results.\n",
    "\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Watter15'></a>\n",
    "# Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images (2015)\n",
    "*Manuel Watter Jost Tobias Springenberg Joschka Boedecker Martin Riedmiller* [pdf](https://pdfs.semanticscholar.org/21c9/dd68b908825e2830b206659ae6dd5c5bfc02.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    " - learning a state?\n",
    " - learning from control\n",
    "\n",
    "### Goal\n",
    "E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.\n",
    "\n",
    "### Method\n",
    "\n",
    "### Related Work\n",
    "- Survey  on representation learning for control  <br>\n",
    "**Autonomous learning of state representations for control** <br>\n",
    "- deep autoencoders (ignoring state transitions)<br>\n",
    "**Deep auto-encoder neural networks in reinforcement learning**<br>\n",
    "- control based on image streams <br>\n",
    "**Human-level control through deep reinforcement learning**\n",
    "- kernel based\n",
    "**Learning  of  non-parametric  control  policies  with  high-dimensional state features**\n",
    "- deep policy learning for robot control\n",
    "**End-to-end training of deep visuomotor policies.**\n",
    "- **From pixels to torques:  Policy learning with deep dynamical models**\n",
    "*Autoencoders are used to extract a latent representation for control from images, on which a non-linear model of the forward dynamics is learned. Their model is trained jointly and is thus similar to the non-linear E2C variant in our comparison. In contrast to our model, their formulation requires PCA pre-processing and does neither ensure that long-term predictions in latent space do not diverge, nor that they are linearizable.*\n",
    "- VAE family : <br>\n",
    "    - **Auto-encoding variational bayes**\n",
    "    - **Stochastic backpropagation and approximate inference in deep generative models.**\n",
    "    - ** DRAW: A recurrent neural network forimage generation.**\n",
    "    - **Learning stochastic recurrent networks.**\n",
    "- enforcing desired transformations in latent space during learning (such that the data becomes easy to model) <br>\n",
    "    - transforming auto-encoders : **Transforming auto-encoders.**\n",
    "    -  probabilistic models for images : **Nice: Non-linear independent components estimation.** AND **ransformation properties of learned visual representations.**\n",
    "-  learning relations between pairs of images : <br>\n",
    "    - **Dynamical binary latent variable models for 3d human pose trackin**\n",
    "    - **Learning to relate images**\n",
    "- state estimation in Markov decision processes\n",
    "    - for a discussion see : **Learning nonlinear dynamic models**\n",
    "    - **Bayesian Forecasting and Dynamic Models**\n",
    "    - **Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models**\n",
    "    \n",
    "\n",
    "### Experiment\n",
    "\n",
    "We evaluate our model on four visual tasks: an agent in a plane with obstacles, a visual version of the\n",
    "classic inverted pendulum swing-up task, balancing a cart-pole system, and control of a three-link\n",
    "arm with larger images.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We presented Embed to Control (E2C), a system for stochastic optimal control on high-dimensional image streams. Key to the approach is the extraction of a latent dynamics model which is constrained to be locally linear in its state transitions. An evaluation on four challenging benchmarks revealed that E2C can find embeddings on which control can be performed with ease, reaching performance close to that achievable by optimal control on the real system model\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Munk16'></a>\n",
    "# Learning State Representation for Deep Actor-Critic Control (2016)\n",
    "*Jelle Munk, Jens Kober and Robert Babuska* [pdf](http://www.jenskober.de/MunkCDC2016.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- predict next state\n",
    "- predict next reward\n",
    "\n",
    "### Goal\n",
    "\n",
    "In this paper, a  new  algorithm,  Model  Learning  Deep  Deterministic  Policy Gradient (ML-DDPG), is proposed that combines RL with state representation learning, i.e., learning a mapping from an input vector  to  a  state beforesolving  the  RL  task\n",
    "\n",
    "### Method\n",
    "\n",
    "The  ML-DDPG algorithm  uses  a  concept  we  call predictive priors to  learn  a model  network  which  is  subsequently  used  to  pre-train  the first  layer  of  the  actor  and  critic  networks.\n",
    "The  ML-DDPG  architecture  consist  of  three  DNNs,  a model network, a critic network and an actor network. The model network is trained by using a new concept that we call predictive priors and  is  integrated  with  the  actor  and  critic networks by copying some of its weights. \n",
    "\n",
    "the *predictable transition* prior which  states that,  given  a  certain  state $s_t$ and  an  action a t taken  in  that state,  one  can  predict  the  next  state $\\hat{s}_{t+1}$\n",
    "\n",
    "The second prior is the predictable reward prior which states that, given a  certain  state $s_t$ and  an  action $a_t$ taken  in  that  state,  onecan  predict  the  next  reward $\\hat{r}_{t+1}$. This  prior  enforces  that all  information  relevant  to  the  task  is  available  in  the  state, which  helps  the predictable transition prior to  converge  to a meaningful representation for the given task.\n",
    "\n",
    "### Related Work\n",
    "- uto-encoder is used to find an observation to state mapping  in  which  the  observations  are  compressed  into  a  low-dimensional  state  vector \n",
    "**Autonomous reinforcement learning on raw visual input data in a real world application,** <br>\n",
    "**Learning  visual  feature  spaces  for  robotic  manipulation  with  deep spatial autoencoders** <br>\n",
    "**Learning  deep dynamical models from image pixels** <br>\n",
    "- Slow feature analysis  used   to   learn   a   mapping   between   visual observations and a state representation that gradually changes over time\n",
    "**Reinforcement  learning on  slow  features  of  high-dimensional  input  streams** <br>\n",
    "**Learning  visual  feature  spaces  for  robotic  manipulation  with  deep spatial autoencoders** <br>\n",
    "- Robotics Prior (Jonschowsky)\n",
    "\n",
    "- predict $\\hat{o}_{t+1}$ instead of $\\hat{s}_{t+1}$\n",
    "**Efficient model learning methods for actor-critic control**\n",
    "**Learning grounded relational symbols from continuous data for abstract reasoning**\n",
    "\n",
    "### Experiment\n",
    "\n",
    "the 2-link arm problem : **The  importance of  experience  replay  database  composition  in  deep  reinforcement learning,**\n",
    "The octopus problem : **Learning  to  control  an octopus arm with Gaussian process temporal difference methods**\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Simulation  results show that the ML-DDPG can learn reasonable continuous control  policies  from  high-dimensional  observations  that  contain also  task-irrelevant  information.  Furthermore,  in  some  cases,this  approach  significantly  improves  the  final  performance  in comparison  to  end-to-end  learning.\n",
    "\n",
    "*********************************\n",
    "\n",
    "\n",
    "Use of the \"predictive prior to learn a task related state representation as a pretraining for the **ML-DDPG** algorithm (Model Learning Deterministic Policy Gradient).\n",
    "\n",
    "the prior aim to predict the next state and reward. It is supervised. :\n",
    "$L_m = ||  s_{t+1} - \\hat{s_{t+1}}  ||^2_2    +  \\lambda_m ||  r_{t+1} - \\hat{r_{t+1}}  ||^2_2 $\n",
    "\n",
    "\n",
    "it allows to train RL faster afterward. ( \"“End-to-end  training of deep visuomotor policies,\" claim it is not possible?)\n",
    "\n",
    "[Back To Outline](#Outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hoof16'></a>\n",
    "# Stable reinforcement learning with autoencoders for tactile and visual data (2016)\n",
    "*van Hoof, Herke, et al* [pdf](https://brml.org/uploads/tx_sibibtex/Hoof2016.pdf)\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning state representation\n",
    "- learning next state\n",
    "\n",
    "\n",
    "### Goal\n",
    "we have introduced a reinforcement learning system consisting of two parts: an autoencoder that represents high-dimensional sensor representations in a compact space, and a reinforcement learner that is able to learn stable, non-linear policies. These stable updates allow on-policy learning with relatively small batches of data, making it possible to adapt the neural encoding during learning.\n",
    "\n",
    "### Method\n",
    "\n",
    "Auto-encoders specialize in finding compact representations, where defining such a metric is  likely  to  be  easier.  Therefore,  we  propose  a  reinforcement learning algorithm that can learn non-linear policies in contin- uous state spaces, which leverages representations learned using auto-encoders.\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    "In our experiments, first we compared different types of autoencoder on a simulated visual task. We found that better results were obtained for this task with variational autoencoders compared to the more traditional de-noising autoencoders. Modifying the objective to reproduce the sys- tem dynamics, rather than encode individual input patterns, also tended to improve reinforcement learning performance although the effect was not as pronounced. Re-training the encoders on the state distribution induced by the policy markedly improved performance. In this case, the encoder objective incurs the biggest loss for states that are most relevant to the policy, and therefore, such states are likely to be represented especially well in the learned feature space. In a second set-up, we considered a real-robot manip- ulation task based on tactile feedback. In this scenario, we showed that the robot was able to learn a policy that manipulates and stabilizes a platform. Rather then using joint encoders or handcrafted features, our algorithm learned the tasks based on complex and high dimensional tactile representations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For both tasks, we noticed that learning from the raw data tends to perform especially poorly in the presence of noise. Distances in the learned feature space are not distorted by noise as much as those in the image space, as the learned feature representation tends to average over multiple input channels. Thus, reinforcement learning on the learned feature space is still successful for the noisy task. We consider this real-robot task to be a first step towards more complex manipulation tasks, where tactile feedback has the potential to improve robustness with respect to perturbances and inaccuracies. We are currently working on learning manipulation policies on multi-fingered robotic hands. In future work, we plan to investigate efficient explo- ration strategies that are critical for success in this domain.\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Spatial Autoencoders for Visuomotor Learning\n",
    "*Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel* [pdf](https://arxiv.org/abs/1509.06113)\n",
    "\n",
    "<a id='MyID'></a>\n",
    "\n",
    "### Classification for overview\n",
    "\n",
    "- learning a state\n",
    "\n",
    "### Goal\n",
    "\n",
    "We present an approach that  automates  state-space  construction  by  learning  a  state representation  directly  from  camera  images. \n",
    "\n",
    "### Method\n",
    "\n",
    "Our  method  uses\n",
    "a  deep  spatial  autoencoder  to  acquire  a  set  of  feature  points that  describe  the  environment  for  the  current  task,  such  as the  positions  of  objects,  and  then  learns  a  motion  skill  with these  feature  points  using  an  efficient  reinforcement  learning method  based  on  local  linear  models.\n",
    "\n",
    "### Related Work\n",
    "- link between the papers<br>\n",
    "**Paper** [pdf](link) <br>\n",
    "\n",
    "\n",
    "### Experiment\n",
    "\n",
    " The  resulting  controller\n",
    "reacts  continuously  to  the  learned  feature  points,  allowing  the robot  to  dynamically  manipulate  objects  in  the  world  withclosed-loop  control.  We  demonstrate  our  method  with  a  PR2 robot  on  tasks  that  include  pushing  a  free-standing  toy  block, picking  up  a  bag  of  rice  using  a  spatula,  and  hanging  a  loopof  rope  on  a  hook  at  various  positions.  \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In  each  task,  our method automatically learns to track task-relevant objects andmanipulate  their  configuration  with  the  robot’s  arm.\n",
    "\n",
    "[Back To Outline](#Outline)\n",
    "*********************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
