Other_scope


% \subsection{Goal exploration and other Intrinsic motivation approaches }

% Over the last decade, intrinsically motivated learning (also called “curiosity-driven learning”) has been studied as an approach to autonomous lifelong learning in machines. Intrinsic motivations are inspired by human ability to discover how to produce “interesting” effects on the environment driven by self-generated motivational signals not related to specific tasks or instructions. The research in this field aims to develop agents that acquire, under the guidance of intrinsic motivations, repertoires of diverse skills that are likely to become useful later when specific tasks need to be performed. A simple analogy, as OpenAI puts it in the case of using HER (Hindsight Experience Replay)\cite{Andrychowicz17}, it is similar to the case when an agent explores and has not a explicit reward (such as food or money are for humans), and while it may not find a gas station he was searching for, however it may learn along the way where a a good pizzeria is \cite{Peng17}. Therefore, a large branch of the literature explores the area of intrinsic motivation and open ended development and learning taking animals and humans as inspiration to provide learning capabilities to robots \cite{Baldassarre14}\cite{Cully15,Kulkarni16}. Leveraging knowledge about how to perform tasks learned during exploration can help achieve tasks later on.

% Examples of curiosity-driven goal exploration architectures include the self-supervision architecture of \cite{Pathak17}, GRAIL (A Goal-Discovering Robotic Architecture for Intrinsically-Motivated Learning) \cite{Santucci16} as well as intrinsically motivated Goal Exploration Processes (IMGEP) with automatic curriculum learning \cite{Forestier17}.  Also the DREAM project cognitive architecture \cite{Salgado16}\footnote{DREAM Project \url{www.robotsthatdream.eu}} provides a motivational engine with autonomous sub-goal identification and a long term memory structure ;% for the Multilevel Darwinist Brain
% while more architectures can be seen in \cite{Baldassarre14}.  %cite does not exist, better one? Duro, R. J., Becerra, J. A., Monroy, J. “Design of a Long Term Memory Structure for the DREAM Project Cognitive Architecture” IMOL17 

%The IMGEP algorithmic architecture relies on several principles: 1) self-generation of goals as parameterized reinforcement learning problems; 2) selection of goals based on intrinsic rewards; 3) exploration with parameterized time-bounded policies and fast incremental goal-parameterized policy search; 4) systematic reuse of information acquired when targeting a goal for improving other goals. We present a particularly efficient form of IMGEP that uses a modular representation of goal spaces as well as intrinsic rewards based on learning progress. We show how IMGEPs automatically generate a learning curriculum within an experimental setup where a real humanoid robot can explore multiple spaces of goals with several hundred continuous dimensions. While no particular target goal is provided to the system beforehand, this curriculum allows the discovery of skills of increasing complexity, that act as stepping stone for learning more complex skill.

Other option discovery useful approach in goal exploration is in \cite{Machado17a} with Proto-Value Functions (PVFs): they implicitly define options.  
In their words: %We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment’s rewards intoconsideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.


% \subsection{One and Few-shot learning}
% Relevant?
%\cite{Woodward17} imitation and meta-learning: https://arxiv.org/pdf/1709.04905.pdf
%https://arxiv.org/pdf/1612.00429.pdf

\subsection{Imitation Learning}
(go intro)\\
%Natalia summarizes: the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised
% classification of the demonstrator’s actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores \cite{Hester17}



\subsection{Symbolic and relational approaches to RL}


\cite{George17}. %Natalia summarize: Recursive Cortical Network (RCN): a generative model that satisfies the functional requirements listed in Table 1 and achieves strong performance and high data efficiency on a diverse set of computer vision tasks. RCN represents a departure from the prevailing deep learning zeitgeist that prizes learning from scratch, tabula rasa. RCN begins with “scaffolding”, prior structure that facilitates model building. For example, while most CNNs and VAEs are whole-image models that assume very little about objects and images, RCN is an object-based model that assumes factorization of contours and surfaces, and objects and background. RCN also represents shape explicitly, and the presence of lateral connections allows it to pool across large transformations without losing specificity, thereby increasing its invariance. Compositionality allows RCN to represent scenes with multiple objects while only requiring explicit training on individual objects. All of these features of RCN derive from our assumption that evolution has endowed the neocortex with similar scaffolding that makes it easy to learn representations in our world compared to starting from a totally blank slate.
%With the right scaffolding in place, learning and inference become far easier. During learning, RCN is much more data efficient than its tabula rasa counterparts – 300x more in the case of a scene text recognition benchmark. Where many models will overfit to extraneous details of their training set, RCN identifies the salient aspects of a scene, permitting strong generalization to other similar scenes. Moreover, in the RCN setting, classification, detection, segmentation, and occlusion reasoning are all different, interconnected queries on the same model that provide explanations for the evidence present in the image. % from nice blog https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/
