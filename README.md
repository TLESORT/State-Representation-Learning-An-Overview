


# State-Representation-Learning(-In-Robotics): An-Overview

[arXiv](https://arxiv.org/abs/1802.04181)

# Related Survey

- **Autonomous learning of state representations for control** (2015) <br>
*Wendelin Bohmer Jost Tobias Springenberg Joschka Boedecker Martin Riedmiller Klaus Obermayer* [pdf](http://www.ni.tu-berlin.de/fileadmin/fg215/articles/boehmer15b.pdf#cite.Lagoudakis03)

## Abstract

Representation learning algorithms are designed to learn abstract features that characterize data.
State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent.
As the representation learned captures the variation in the environment generated by agents, this kind of representation is particularly suitable for robotics and control scenarios.
In particular, the low dimension helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning.
This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.


## Outline
:one: Learning a State <br>
:two: Learning next state (transition) - forward model <br>
:three: Learning to construct next image (or observation) <br>
:four: Learning a metric <br>
:five: Applications <br>
:six: Validation Methods and Frameworks <br>

:pencil: resume written

# Learning objective for SRL (state representation learning)

 :one: Learning by reconstruction the observation <br>
 :two: Learning a Forward model <br>
 :three: Learning an Inverse Model <br>
 :four: Using feature adversarial learning <br>
 :five: Exploiting reward <br>
 :six: Other objective functions <br>
 :seven: Using hybrid objectives <br>



## :one: Learning by reconstruction the observation

- **Deep Spatial Autoencoders for Visuomotor Learning**<br> *Finn, Chelsea, et al.*, (2015)

- **Goal-Driven Dimensionality Reduction for Reinforcement Learning** (2017) *Parisi17*

- **Disentangling the independently controllable factorsof variation by interacting with the world** *Thomas17b*


- **Independently Controllable Factors** *Thomas17*

- **Learn to swing up and balance a real pole based on raw visual input data** *Mattner12*

- **Dimensionality Reduced Reinforcement Learning for Assistive Robots** *Curran16*

- **Using PCA to Efficiently Represent State Spaces** *Curran15*




## :two: Learning a Forward model

 - **Deep Kalman Filters** (2015) <br>
 *Rahul G. Krishnan, Uri Shalit, David Sontag*, [pdf](https://arxiv.org/abs/1511.05121) [arXiv](https://arxiv.org/abs/1511.05121) [bib](http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2015arXiv151105121K&data_type=BIBTEX&db_key=PRE&nocookieset=1)


- **Learning to linearize under uncertainty** (2015) <br>
*R. Goroshin, M. Mathieu, and Y. LeCun* [pdf](https://arxiv.org/pdf/1506.03011.pdf) [arXiv](https://arxiv.org/abs/1506.03011) [bib](http://dblp.uni-trier.de/rec/bibtex/journals/corr/GoroshinML15)

- **Embed to control: A locally linear latent dynamics model for control from raw images** (2015) <br>
 *Watter, Manuel, et al* [pdf](https://pdfs.semanticscholar.org/21c9/dd68b908825e2830b206659ae6dd5c5bfc02.pdf) [arXiv](https://arxiv.org/abs/1506.07365) [bib](http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2015arXiv150607365W&data_type=BIBTEX&db_key=PRE&nocookieset=1)

- **Learning State Representation for Deep Actor-Critic Control**. Jelle Munk 2016. [pdf](http://www.jenskober.de/MunkCDC2016.pdf)

- **Stable reinforcement learning with autoencoders for tactile and visual data.**<br> *van Hoof, Herke, et al*, (2016) [pdf](https://brml.org/uploads/tx_sibibtex/Hoof2016.pdf)

- **Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data** , *Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der Smagt*, (2017),  [pdf](https://openreview.net/pdf?id=HyTqHL5xg) [arXiv](https://arxiv.org/abs/1605.06432) [bib](http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2016arXiv160506432K&data_type=BIBTEX&db_key=PRE&nocookieset=1)

- **Value Prediction Network** TODO

- **How can a recurrent neurodynamic predictive coding model cope with fluctuation in temporal patterns? Robotic experiments on imitative interaction** (2017) *Amhmadi17*  TODO

- **Predictive Coding for Dynamic Visual Processing: Development of Functional Hierarchy in a Multiple Spatio-Temporal Scales RNN Model** (2017) *Choi17* TODO

- **Data-efficient learning of feedback policies from image pixels using deep dynamical model** (2015) *Assael*

- **Learning deep dynamical models from image pixels** (2014) *Wahlstrom14*

- **From pixels to torques: Policy learning withdeep dynamical models** (2015) *Wahlstrom15*

- **Curiosity-driven reinforcement learning with homeostatic regulation** (2017) *Magrans17*

## :three: Learning an Inverse Model

- **Loss is its own Reward: Self-Supervision for Reinforcement Learning** (2016) <br>
 *Evan Shelhamer, Parsa Mahmoudieh, Max Argus, Trevor Darrell* [pdf](https://arxiv.org/pdf/1612.07307.pdf) [arXiv](https://arxiv.org/pdf/1612.07307.pdf)

- **Curiosity-driven Exploration by Self-supervised Prediction** (2017) *Deepak Pathak et al.* [pdf](http://juxi.net/workshop/deep-learning-robotic-vision-cvpr-2017/papers/23.pdf)
Self-supervised approach.

## :four: Using feature adversarial learning

- **InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets** (2016) <br>
 *Xi Chen, Yan Duan, Rein Houthoof, John Schulman, Ilya Sutskever, Pieter Abbeel * [pdf](https://arxiv.org/pdf/1606.03657.pdf)
 


## :five: Exploiting reward
## :six: Other objective functions

- **PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations**, <br>
 *Rico Jonschkowski, Roland Hafner, Jonathan Scholz, Martin Riedmiller*, (2017), [pdf](https://arxiv.org/pdf/1705.09805), [arXiv](https://arxiv.org/abs/1705.09805) [bib](http://dblp.uni-trier.de/rec/bibtex/journals/corr/JonschkowskiHSR17)

 - **Learning State Representations with Robotic Priors**  (2015) <br>
 *Rico Jonschkowski, Oliver Brock*, , [pdf](https://pdfs.semanticscholar.org/dc93/f6d1b704abf12bbbb296f4ec250467bcb882.pdf) [bib](http://dl.acm.org/citation.cfm?id=2825776)


## :six: Using hybrid objectives







# Citation

If you find this repo useful please cite the relevant paper <br>

```
@ARTICLE{2018arXiv180204181L,
   author = {{Lesort}, T. and {D{\'{\i}}az-Rodr{\'{\i}}guez}, N. and {Goudou}, J.-F. and 
	{Filliat}, D.},
    title = "{State Representation Learning for Control: An Overview}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1802.04181},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
     year = 2018,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180204181L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
```

