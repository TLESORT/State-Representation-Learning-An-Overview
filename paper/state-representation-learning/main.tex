\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{longtable} 
\usepackage{rotating} 
%\usepackage{pdflscape} %paper, not table in landscape rotated mode

\newcommand{\david}[1]{\todo[color=blue!60,inline]{#1}}
\newcommand{\natalia}[1]{\todo[color=green!60,inline]{#1}}
\newcommand{\tim}[1]{\todo[color=yellow!60,inline]{#1}}

\title{State Representation Learning: An Overview}
\author{the firemakers}

\begin{document}
\maketitle

\begin{abstract}
Deep learning is ubiquitous, and robotics could not be a less deserved area worth penetrating. Soon robots will be able to benefit from deep learning frameworks to make complex choices and predictions in a robust and autonomous fashion. The interesting particularity of a robot is that its input perception is, as in humans, multimodal. The robot can use its camera,  LIDAR, radar, microphone, etc., while it executes actions. As in many other traditional problems that deep learning is targeting, the classic challenge of the curse of dimensionality remains. A key issue is how to make an algorithm able to make prediction with several high dimension inputs and how make it find hidden dependencies between them online. A solution is to reduce the dimensionality of each input by learning a mapping into a state representation. State representation learning means find the few hidden parameters of each of the input (or modularity). Once the hidden parameters are found the task of finding dependencies between modularities is no more bothered by the dimensionality. This paper aims to cover the state of the art about state representation learning. It presents the different methods used to disentangle the hidden parameters of an datasets and to validate the learned state representation. This overview is particularly focused on learning representations in low dimensionality of known parameters such as, for instance, %like 
the state or position of a 3D object. This scope makes possible to assess the representation learned.
\end{abstract}

\section{Introduction}
The use of deep learning in autonomous robots will be determinant in a near future, for example for autonomous vehicles. The data from hundreds of sensors of an autonomous robot has to  be analysand and transformed into meaningful representations. Moreover, the need of validation and interpretability for those representations will be key for the acceptance of deep learning technologies in our lives. 
Reinforcement learning is one of the most used approaches to train neural networks for autonomous tasks (\cite{Mnih15}, \cite{Lillicrap15}, \cite{Schulman15}, \cite{Wu17}, \cite{Schulman17}). The problem of these approaches is that they are very data consuming, in particular if the data is high dimensional. Furthermore, the most data is needed for reinforcement learning, the more time it takes to train. Learning to map data to compact representations is key to make reinforcement learning faster and more data efficient. Learning this mapping allows to produce state representations of the data produced by sensors, in particular from camera. A state representation is a low dimension vector which characterizes the whole state of the data. Learning state representation can be seen as a dimensionality reduction method. Learning a state is implicitly done in the so-called end-to-end reinforcement learning. % dont start sentences with But, use However
However, taking the task of representation learning away from reinforcement learning may accelerate reinforcement learning, make easier the training process, and make it more robust. [citations here]
\cite{Bohmer15} present the state of the art in terms of learning state representations for control. They present approaches based on deep auto-encoder networks followed by approaches on slow feature analysis (SFA). %There 
Their vision of the notion of a good state representation is that it should: % this sentence should be readable with either of the bullet points with verb in sync
\begin{itemize}
\item Be Markovian
\item Be able  to represent  the  true  value of  the current policy well enough for policy improvement
\item Generalize the learned value-function to unseen states with similar futures
\item Be low dimensional for efficient estimation
\end{itemize}

The survey in \cite{Bohmer15} shows recent  approaches in the field of state representation learning, and it is particularly focused on learning state representations from images. 

The rest of this article is structured as follows. First we present approaches which learn a representation for a time step $t$; secondly, we present approaches that use predictions in the future to learn a state representation, and finally, we present validation methods, frameworks and benchmarks used to assess the quality of the learned representations.


A broad branch of (generally unsupervised) approaches are mainly learned thanks to physics related or energy related (auto-encoders) functions, and they will be described in Section \ref{Sec:UnsupervisedMethods}.  Another possibility is to find the representation which make possible actions or which maximize rewards. The representation would then be learned thanks to interaction with the data (or with the world which is the same) and reward functions. Those representation can be learned through a reinforcement learning algorithms, some of which will be summarized in this section.

% \subsubsection{Relational and semantic models}
% More traditional AI and relational models learn concepts such as entities of the world, their attributes, and relational properties to reason about the world.
% In this area, an example of novel approach is in \cite{Garnelo16}, where they proposed an end-to-end reinforcement learning architecture where a neural back end must learn a symbolic representation of its raw data to communicate with the front end. They handle three main components of neural-symbolic hybrid systems: 1)Conceptual abstraction. 2) Compositional structure. 3) Common sense priors, i.e., one of the first works bridging the gap among logics and neural models.

% Relational Networks \cite{Santoro17} and Visual Interaction Networks \cite{Watters17} are two philosophically similar models that use abstract logic to reason about the world. Relational reasoning is very closely linked to the elusive human "common sense", something for a long time not thought as being capable of doing by other animals (e.g., \textit{What is the color of the object closest to the red square?, How many objects have the same shape as the blue one?}). Relational learning can also be done via programmable agents, where differentiation happens through program execution \cite{Denil17}, as a way to avoid catastrophic forgetting when learning new tasks.

% \subsection{Learning a metric} 
%  A survey including an overview of metric learning approaches for feature vectors can be seen in \cite{Bellet13}. %https://arxiv.org/pdf/1306.6709) 
% An example of metric learning for control is in \cite{Watter15}, where  a locally linear latent dynamics model is used to learn an embedding for control from raw images. 

\section{Unsupervised state representation learning}
\label{Sec:UnsupervisedMethods}

  A common approach to teach agents useful tasks such as robotics control is via reinforcement learning.  In this context, agents need to solve a perspective-taking task that requires the agent to consider the perceptual state of another. States can also be seen outside RL context, e.g., within the Theory of mind, which is the ability to assign distinct mental states (beliefs, intents, knowledge, etc.) to other members\footnote{Computational Neuroscience \url{http://neuro.cs.ut.ee/lab/}}. We will focus on this paper on the former context mainly.

In the field of learning state representations and reinforcement learning for agents to interact and perform control in the environment, different strategies have been used. This part present strategies that learn a mapping from observation at time t $o_t$ to a state representation $s_t$. The trainings presented are unsupervised or weakly supervised. We support the believe that learning a state should not be supervised because the risk of overfitting is high \cite{Bengio12}. 

\subsection{Learning states with Auto-encoders }
In order to learn state representation, one common strategy is the use of an auto-encoder. The architecture of the auto-encoder impose a constraint of low dimensional with a bottleneck between encoder and decoder. The assumption is : the better way to compactly synthesize an image is to learn a representation of the scene. Thus the encoder $E$ learn a mapping from the input to the state $s_t$ and the decoder $D$ learn to reproduce the input $x$ image to the output $\hat{x}$. The error is the squared error computed pixel-wisely between the input and the output.
\begin{equation}
L = \parallel x - \hat{x} \parallel^2 \mbox{ ,}
\label{equation_Squared_error}
\end{equation}
\natalia{why this comma?}


Using this loss function the encoder and decoder can be trained by gradient descent. The auto-encoder architecture can then be adapt to learn a particular representation. \cite{Finn15} use a spatial auto-encoder to learn spatial state representation of a controller. The representation learn is after use to learn manipulation skills.

Unfortunately the even if theoretically the best solution is to learn a state representation, the training often lead to a "mean" image which optimize quite good the loss for every input, focusing on main visual feature and ignoring small but relevant ones \cite{Lesort17}. \\
To make the training more robust to this kind of false optimization solution, we can use a denoising auto-encoders (DAE) \cite{Vincent08}. This architecture add noise to the input. This will make the "mean" image a worse solution than before. This architecture is experimented in \cite{Hoof16} to learn visual and tactile state representation. The authors compared state representation learned by DAE and variational auto-encoder (VAE) ( \ref{subsec:vae} ) by achieving reinforcement learning on it. They found that DAE state representation make it possible to get less rewards than VAE state representation in most of the case. \\

\subsection{Variational Learning}
\label{subsec:vae}
In the field of state representation learning, an other solution explored is the use of a variational inference \cite{Kingma13}, \cite{Rezende14} to learn a  the state representation. In this context the variational inference is particularly use for variational auto-encoder as they learn to compress data into low dimensional representations.

The hidden parameters of a environment can be interpreted as latent variable $z$ of a distribution $p$ which generate the input data $x$. We can find those latent variables by fitting $p(x,z)$ with a distribution $q(x,\hat{z})$. The variational approach aims to find this distribution $q$ to extract a representation \cite{Watter15}. The problem is that $p(x)$ is intractable as $p(z|x)$. which make the use of variational auto-encoders interesting.\\ 

The variational auto-encoder learn an approximate distribution $q(z|x)$ to fit the intractable true posterior $p(z|x)$. This distribution is called a probabilist encoder (or recognition model). It learn a mapping from an input to a representation $\hat{z}$. On the same way $p(x|z)$ is called probabilist decoder : given a $z$ it produce a sample x of the distribution $p$.

The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints $log p(x^{(1)},...,x^{(N)})= \sum_{i=1}^N \log{p(x^{(i)})}$, can each be rewritten as :

\begin{equation}
\log{p(x^{(i)})} = D_{KL}(q(z|x)\parallel p(x,z))+\mathcal{L}(x^{(i)};\theta) \mbox{ ,}
\label{equation_log_likelihood}
\end{equation}

Where $\theta$ indicates the parameters of $p$ and $q$ models. The first term is the KL divergence of the approximate from the true posterior.   Since this KL-divergence is non-negative, the second term is called the variational lower bound on the marginal likelihood of datapoint i.\\
The optimization of the lower bound can be achieved efficiently thanks to the re-parametrization trick of q(z|x) \cite{Kingma13}, \cite{Rezende14}. By re-arranging the lower bound we get:

\begin{equation}
\mathcal{L}(x^{(i)};\theta) = \mathbf{E}_{q(z|x)}[\log{p(x,z)}- \log{q(z|x)}] \leq \log{p(x^{(i)})} \mbox{ ,}
\label{equation_lower_bound}
\end{equation}

\begin{equation}
 = \mathbf{E}_{q(z|x)}[\log{p(x|z)}- D_{KL}(q(z|x)\parallel p(z))] \mbox{ ,}
\label{equation_lower_bound2}
\end{equation}

The first term is called negative reconstruction error while the second one is a regularizer error.\\
The re-parametrization trick is a alternative method for generating sample from $q(z|x)$. Let be z a continuous random variable sampled from $q(z|x)$, we express z as a deterministic variable $z=g(\epsilon, x)$, where $\epsilon$ is auxiliary variable with independent marginal $p(\epsilon)$ and $g(.)$ is the function learn by the encoder.\\
The VAE implementation of this trick gives that the encoder predict a vector of $\mu$ and $\sigma$ then $\epsilon$ is sampled as $\epsilon \sim \mathbf{N}(0,I)$. $z$ is computed by :

\begin{equation}
 z = \mu * \sigma + \mu \mbox{ ,}
\label{equation_reparametrization_trick}
\end{equation}

It gives a fully differentiable way to produce a univariate Gaussian distribution $p(x|z)=\mathbf{N}(\mu,\sigma)$. Therefor optimization of the lower bound is feasible by gradient descent.

%presentation "Embedded To Control"
 \cite{Watter15} presented a VAE model called "Embedded To Control" (E2C). It consists of a deep generative model that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. The latent space is task related. They evaluate their model on four visual tasks: an agent in a plane with obstacles, a visual version of the classic inverted pendulum swing-up task, balancing a cart-pole system, and control of a three-link arm with larger images. While \cite{Watter15} learned control policies in a single shot based on data under an exploration policy, \cite{Hoof16} aims, using also VAE, to learn iteratively on-policy. The authors want to train a state encoders in a way that respects the transition dynamics of the controlled system. This transition dynamics used to learn a representation make \cite{Hoof16} part on forward model (subsection \ref{subsec:forward} )

Variational inference are also use in Deep Kalman Filters (\cite{Krishnan15}) to construct a continuous nonlinear-state space model. Deep Kalman filters (DKF) are an evolution to Kalman filter which use variational methods to replace all the linear transformations with non-linear transformations parameterized by neural nets. \cite{Krishnan15} apply DKF to model the “Healing MNIST” dataset where long-term structure, noise and actions are applied to sequences of digits. 


This approximation of the posterior have also been used to extend Kalman filters into deep variational Bayes Filter (DVBF)in \cite{Karl16}. From  raw non-Markovian sequence data the authors create a nonlinear state space. The paper demonstrates that this approach can be use to estimate state of a pendulum and a bouncing ball.
%- **Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data** extend summary or enough?

\subsection{ Learning with priors}
% transition to prior
As VAE are auto-encoder the error is compute in order to reduce the distance between the input and the output. The learned representation is then not necessary related to the state we want to find. If the state is not a main visual feature of the reconstruction, an auto-encoder will not learn to reproduce it in priority. Another approach is then to not use the reconstruction of the image as criterion but to use knowledge we have about the state representation we want to learn a use it to search it inside the data. Those a priori knowledge are called " priors". As described in \cite{Bengio12}, and contrarily to Bayesian probability in the context of state representation learning, a prior is not an a priori distribution but an a priori knowledge we have about the world. Most priors are concerned about the physics of the world \cite{Scholz14}. This knowledge help to learn a dense and efficient representation about the world. They can be use to learn low dimensional representation in a robotics set up \cite{Jonschkowski15}, or in simulation \cite{Jonschkowski17}.

Different priors have been proposed to address the problem of sharing knowledge with neural network. There are often related to the time. In fact for instance one of our best advantage against neural neural is our comprehension of the interaction between entities through time. The neural network have a lot of difficulties with long term dependencies. Explaining that information can easily be extract from time by looking at it the right way would help significantly a neural network to understand and make decisions. It could also reduce the complexity of the inference because there is a lot of coherence through time. Here is a list of the priors and the paper which formulate it or use it.\\

\begin{itemize}
\item  \textbf{Simplicity}\\
The simplicity prior is implemented in most state representation learning papers. This prior assume that it exits a low dimensional representation of an high level input. This low dimensional representation is the one we want to learn.\\
This prior is implemented in all the paper we speak about because it impose a low dimensional state representation. The prior is implemented in the architecture of the neural network where a bottleneck force the representation to be compact.


\item \textbf{Slowness Principle}\\
The slowness principle assume that interesting feature fluctuating slowly and continuously through time and that a radical change inside the environment has low probability \cite{Wiskott02,Kompella11}. It's also called time coherence or the prior of 

\begin{equation}
L_{Slowness}(D,\hat{\phi})=\mathbf{E}[\parallel\Delta\hat{s}_{t}\parallel^2] \mbox{ ,}
\label{equation_Prior_Temporel}
\end{equation}

This prior can have other naming depending on the unit of $s_t$, for example Time Coherence (time) or Inertia (velocity).

\item \textbf{Variation}\\

The assumption of this prior is that positions of relevant objects vary. Learning state representation should then focused on moving objects.

\begin{equation}
L_{variation}(D,\hat{\phi})=\mathbf{E}[ e^{- \parallel \hat{s}_{t1}- \hat{s}_{t2}\parallel}] \mbox{ ,}
\label{equation_Prior_Variation}
\end{equation}

$e^{-distance}$ is use as  a  similarity  measure  that  is 1 if the  distance  is 0 and  that  goes  to 0 with  increasing  distance between the position states, which is exactly what we want

\item \textbf{Proportionality}\\
The proportionality prior assumes that for a same action, the reactions of these actions will have proportional amplitude. The representation will then vary in the same amount for two equal actions in different situation.

\begin{equation}
L_{Prop}(D,\hat{\phi})=\mathbf{E}[(\parallel\Delta\hat{s}_{t_2}\parallel-\parallel\Delta\hat{s}_{t_1}\parallel)^2 | a_{t_1}=a_{t_2}] \mbox{ ,}
\label{equation_Prior_Prop}
\end{equation}

\item \textbf{Repeatability}\\
Two identical actions applied at similar states should provide similar state variations, not only in magnitude but also in direction.

\begin{equation}
L_{Rep}(D,\hat{\phi})=\mathbf{E}[e^{-\parallel\hat{s}_{t_2}-\hat{s}_{t_1}\parallel^2}\parallel\Delta\hat{s}_{t_2}-\Delta\hat{s}_{t_1}\parallel^2 \mid a_{t_1}=a_{t_2}] \mbox{ ,}
\label{equation_Prior_Rep}
\end{equation}

\item \textbf{Causality}\\
The causality prior is concerned with rewards. This prior assumes that if we have two different rewards for two same actions, then the two states should be differentiated apart in the representation space.

\begin{equation}
L_{Caus}(D,\hat{\phi})=\mathbf{E}[ e^{-\parallel\hat{s}_{t_2}-\hat{s}_{t_1}\parallel^2} \mid a_{t_1}=a_{t_2},r_{t_1+1}\neq r_{t_2+1}] \mbox{ ,}
\label{equation_Prior_Caus}
\end{equation}

\item \textbf{Controllability}\\
 Controllable  things  are  relevant for state representation learning. The objects that can be controlled by the robot are likely relevant for its task. If the robot acts by applying forces, controllable things  could  be  those  whose  accelerations  correlate  with  the actions of the robot. Accordingly, we can define a loss function per action dimension i to optimize covariance between action dimension i and accelerations in a state dimension i. 
Related with this prior is the notion of \textit{empowerment}, defined as an information-theoretic capacity of an agent’s actuation channel to influence its own evolution, as a universal agent-centric measure of control. This concept is motivated by classical utility functions, which suffer from having to be designed and tweaked on a use-case basis \cite{Klyubin05}. Encoding priors based on such concept, as well as the assignment problem \cite{Sutton98} is worth exploring when the environment becomes multi-agent.

\begin{equation}
L_{controllability}(D,\hat{\phi})= e^{-cov(a_{t,i},s_{t+1,i})} \mbox{ ,}
\label{equation_Prior_controllability}
\end{equation}


\item \textbf{Consciousness}\\
This prior could be combined with other priors in order to help disentangling abstract factors from each other. It relates to the notion of awareness at a particular time instant, as a \textit{powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions} \cite{Bengio17}. The idea behind aims at making predictions in an abstract richer (but more compact with less dimensions) space than in the pixel space, as a natural mapping from  facts or rules.

\tim{how to use the Consciousness prior? Nat:This is future work and promising direction. DAVID, shall we use the prediction (is it a prediction or loss function?) function in Bengio slides?} % see https://www.dropbox.com/s/kuthfbfxqp8nmg0/bengio.pdf?dl=0

\end{itemize}

Priors can be combined with other approaches such as those in \cite{Finn15} where the slowness principle is combined with the reconstruction of an auto-encoder. Those priors can be used to learn representations in high dimensions such as it is done in \cite{Stewart16}; however, its main objective is beyond the scope of this paper.

\subsection{Adversarial Learning}
\label{GANs}
\cite{Chen16} propose to use the Generative Adversarial Network (GAN) framework to learn state representations. They present a model named InfoGAN that achieves to learn to disentangle latent variables, especially in the context of this paper, on 3D pose of objects. GANs have been presented by \cite{Goodfellow14}. As described in \cite{Chen16}, the goal is to learn a generator distribution $P_G(x)$ that matches the real distribution $P_{data}(x)$. Instead of trying to explicitly assign probability to every $x$ in the data distribution, GAN learns a generators network $G$ that generates samples from the generator distribution $P_G$ by transforming a noise variable $z \sim P_{noise}(z)$ into a sample $G(z)$. This generator is trained by playing against an adversarial discriminator network D that aims to distinguish between samples from the true distribution $P_{data}$ and the generator distribution $P_G$. They do so by maximizing the mutual information between a fixed small subset of the GAN's noise variables and the observations, which turns out to be relatively straightforward.

\tim{need to add other GAN model, probably Bigan for example- see GANs.md}
Adversarial Feature Learning, Donahue17. Presents an extension of regular GANS to learn the inverse mapping: projecting data back into the latent space that allows the learned feature representation to be useful for auxiliary supervised discrimination tasks that is competitive with unsupervised and self-supervised feature learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning by Predicting}
\label{LearningByPredicting}
Theoretical and empirical proofs show that inference for a latent state should be performed using information from its future (i.e., looking at the true posterior), against recent work which mainly performs inference using only information from the past \cite{Chung15,Krishnan17}. \cite{Krishnan17} introduce a new family of structured inference networks (parameterized by recurrent neural networks), that by using Gaussian State Space Models, considers both inference and learning in a class of latent state variable that serves as a summary statistic for the information from the past.
%Not sure what they mean: Our approach may easily be adapted to learning more general generative models, for example models with edges from observations to latent states. 
%Incentivizing the inference network to incorporate the latent information at training time  would allow time-variant latent distributions which is more aligned with generative neural models for time-series \cite{Krishnan17,Babaeizadeh17}. % NAT: Ref to be added in the intro as a motivation for SRL?

A broad branch of approaches focuses on reinforcement learning by using self-supervised approaches, i.e., using the future to either predict the next state to be reached after performing a given action (forward models) or predict the action to reach a desired future state (inverse models). However, predicting other environment signals such as visual or computational features are proxies shown to improve reinforcement learning \cite{Jaderberg16}. This section will summarize this broad spectrum of research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%PREDICT NEXT STATE (%FORWARD MODELS )
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predicting next state: Forward Models }
\label{subsec:forward}
In the reinforcement learning framework the basic elements are based on tuples $(o_t, s_t, a_t, r_t)$ where from observation $o_t$, one can learn a corresponding abstract state $s_t$ in an embedded space, in which performing action $a_t$ returns reward $r_t$.
In this section we present approaches that learn from an observation $o_t$ and an action $a_t$ to predict the next future state $\hat{s}_{t+1}$. This kind of approach learns what is called a \textit{forward} model. In other words, given state and action taken in time $t$, predict next future state at $t+1$ that that action will lead us to. Formally: 

\begin{equation}  
\hat{s}_{t+1} = f(s_t, a_t; \theta_{fwd})
\label{fwd}
\end{equation}

Predicting context is critical for tackling unsupervised learning and being able to generate content. In unsupervised learning, self-supervised training approaches can be used to improve data efficiency without sacrificing return. They are approaches that in lack of labels, they make a pseudo-label out of the prediction error that comes from trial and error prediction of interactions with the environment. Self-supervised approaches can optimize tasks independent of reward, such as dynamics and inverse dynamics-related tasks. One example of this approach is predicting loss \cite{Shelhamer17}, which can be used as a reward when extrinsic rewards are unavailable. In \cite{Shelhamer17}, self-supervised auxiliary losses extend the limitations of traditional RL to learn from all experience, whether rewarded or not. Self-supervised pre-training and joint optimization using auxiliary losses in the absence of rewards improve the data efficiency and policy returns of end-to-end reinforcement learning.  Self-supervision improvement is strongest early in training when the pre-training and policy distributions are close \cite{Shelhamer17}\footnote{The policy gradient is augmented with auxiliary gradients from what is called \textit{self-supervised} tasks. \cite{Shelhamer17} accuses the \textit{gap between the initial optimization and recovery as a representation learning bottleneck in systems that are not end-to-end}}. %End-to-end RL admits policy learning in lieu of policy design in much the same way that end-to-end supervised learning has seen the advance of feature learning over feature design).

\cite{Oh17} integrate model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract (discrete) states are trained to make option-conditional predictions of future values (discounted sum of reward, discount and the value of next state) rather than predictions of future observations. VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Because they outperform Deep Q-Network (DQN) on several Atari games with short-lookahead planning, it can be a potential new way of learning state representations. 
A set of similar approaches integrating learning and planning in one architecture exist. One is the RNN-based \textit{Predictron} \cite{Silver16}, which works in uncontrolled settings  but requires policy evaluation. Another one is Dyna-Q \cite{Sutton90}, but for continuous control problems using model predictive control (MPC) \cite{Lenz15}, or CNN-based Value Iteration Networks (VIN) \cite{Tamar16}, which %performs value iteration approximating the Bellman-update through a CNN 
is constrained to lower dimensionality state spaces.

Another interesting related area concerns learning hierarchical intention states with \textit{Multiple Spatio-Temporal Scales RNN}-based predictive coding \cite{Choi17,Ahmadi17}. % TO EXPAND


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%PREDICTING NEXT ACTION:  INVERSE MODELS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Predicting next action: Inverse Models}

Without leaving the RL setting, and within a self-supervision paradigm of exploring, in order to learn about the environment, we can turn the forward model problem around and instead of learning a representation of states (given previous states and actions), use the states to predict actions.

In an inverse model, given a current state $s_t$  and the next desired state $s_{t+1}$, we learn with supervised learning to predict the necessary action $a_{t+1}$ to reach such future state \footnote{Note that Inverse models are different from inverse RL, where rewards are generally unavailable and given states and actions, a reward function is learned. This area of research has also exploited for transferring learning from simulation to real world, through learning deep inverse dynamics models \cite{Christiano16}. However, we focus on the former, where predicting the action from states can be used to improved the state representation learning}. This is a common problem in planning and navigation in AI, where the goal we want to reach is given and we need to find the actions that take us there. More formally, we learn $\hat{a}_t$:

\begin{equation}
\hat{a}_t = g(s_t, s_{t+1}; \theta_{inv})
\label{inv}
\end{equation}

Equations \ref{fwd} and \ref{inv} summarize forward and inverse models, where $s_{t}$ and $a_{t+1}$ are the world state and action applied time step $t$; $s_{t+1}$, $a_{t+1}$ are the predicted state and actions, and $\theta fwd$ and $W inv$ are parameters of the functions $f$ and $g$ that are used to construct the forward and inverse models.

Inverse models can also be learned via active learning \cite{Baranes13} with intrinsically motivated goal exploration in robots. Connexions among the latter two families of models exists: forward models can regularize inverse dynamics model. %do you know  evident citation?
An example using both forward and inverse models is the Intrinsic Curiosity Module (ICM) \cite{Pathak17}, which helps agents explore and discover the environment out of curiosity when extrinsic rewards are spare or not present at all, and integrates both an inverse and forward model where the 'surprise' element in the action prediction is used as signal for the forward model. This is an interesting way to bypass the hard problem of predicting pixels, unaffected by unpredictable aspects of the environment that do not affect the agent. In the inverse model, the neural network with parameters  $\theta_{inv}$ is trained to optimize: 

\begin{equation}
Loss_{inv}=  min_{\theta_{inv}} L_{inv} (\hat{a_t}, a_t)  
\end{equation}
where, $L_{inv}$ is the loss function that measures the discrepancy between the predicted and actual actions. The intrinsic reward signal is computed from the loss of the forward model:
\begin{equation}
Loss_{fwd} = Loss_{fwd} (\phi(s_t), \hat{\phi}(s_{t+1})) = \frac{1}{2} \parallel \hat{\phi} (s_{t+1}) − \phi(s_{t+1}) \parallel_2 ^2
\end{equation} %Intrinsic reward signal is computed as the difference between the next state and the estimated next state, pondered by a η/2 (η > 0)  scaling factor
%KEY: the overall optimization problem minimizes 4 weighted components: the inverse model loss against the forward model loss, and  the importance of the policy gradient loss against the importance of learning the intrinsic reward signal. 
Their most interesting result is that using an observation space for computing curiosity poorly predicts the best action (in an inverse model) and is significantly worse than learning an embedding. When comparing with an architectures without inverse models (and add deconvolution layers to the forward model), the uncontrolable parts of the environment are ignored and in this way, ICM succeeds on harder exploration tasks that become progressively harder for a baseline such as A3C. %, whereas the curious A3C is able to achieve good score in all the scenarios. %ICM-pixels is close to ICM in architecture but incapable of learning embedding that is invariant to the uncontrollable part of environment.

\natalia{would be worth adding image in their website or avoid because of copyrights (since it its in their website may be ok? Also shall this description be added? from Shelhammer, literally, below: }
% https://pathak22.github.io/noreward-rl/ 
% three kinds of learning by their objectives:
%  supervised learning min E[Ldis(f(x); y)]
%  unsupervised learning min E[Lgen(f(x); x)]
%  self-supervised learning min E[Ldis(f(x); s(x))]
% with surrogate annotation function s()
% for data x, annotations y, losses L either discriminative
% or generative, and parametric model f. Both unsupervised
% learning and self-supervised learning define losses
% without annotation, but unsupervised learning has historically
% focused on generative or reconstructive losses, while
% nascent self-supervised methods instead define surrogate
% losses and synthesize the annotations from the data \cite{Shelhamer17}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%PREDICTING NEXT OBSERVATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Predicting next observation}
%ADD HERE OTHER PREDICTIVE MODELS AND AUXILIARY TASKS?

A branch of approaches learns a mapping from observation $o_t$ to a state $s_{t+1}$ in a supervised manner. Even if the learning criterion does not depend on a state, the architecture of the model allows to extract a state representation as it happens by using auto-encoders. 

Predictive models in general have shown success in robotics, health-care, and video understanding applications \cite{Vondrick17}. Explicitly disentangling the model’s memory from the prediction (by, e.g., transforming pixels in the past as in \cite{Vondrick17} to generate short videos of plausible futures) helps the model learn desirable invariance.  

In unsupervised learning contexts, since the acquisition of labeled data becomes increasingly expensive and impractical, it is possible to learn about physical interaction dynamics with action-conditioned video prediction models \cite{Finn16Unsupervised}. These use pixel motion to predict a distribution over it from previous frames and in this way produce video predictions. 


\subsection{ To be INCLUDED OR NOT? Predicting the reward: Inverse Reinforcement Learning } %%% RELEVANT?   
In inverse reinforcement learning (IRL), no reward function is given; instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic the observed behavior that is often optimal or close to optimal \cite{Ng00}. One example, when rewards are not present or are scarce%\footnote{\url{http://giorgiopatrini.org/posts/2017/09/06/in-search-of-the-missing-signals/}} 
is \textit{Compatible Reward IRL} \cite{Metelli17}, where a model-free approach is presented. It does not require to specify a function space where to search for the expert's reward function; the algorithm generates basis functions that span the subspace that make the gradient vanish, and use this subspace to penalize the policies that deviate the most from the expert's policy. This approach can outperform behavioral cloning and true reward function in finite and continuous cases. %compared to state-of-the-art IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian Regulator and Car on the Hill environments.




\section{Current Research and Challenges }
\label{CurrentChallenges}
The challenge of high dimensionality spaces of actions
\footnote{Approaches in large discrete action spaces have been sought, such as  in \cite{Dulac-Arnold15}} or continuity in action spaces are examples of difficult to tackle issues found in the current trends on representation learning. Some other challenges and current trends on reinforcement learning include:

\tim{we should keep only challenges related to the survey}

\begin{itemize}
\item Generalizing from simulation \cite{Peng17} (the \textit{reality gap}) and  Simulation-to-real transfer learning \cite{Tobin17,Peng17}.
\item Including options \cite{Machado17a} and goal exploration processes \cite{Forestier17} as a way to improve state representation learning.
\item Augmenting the dimensionality and continuity of observation space \cite{Lesort17}.
\item Adversarial feature learning \cite{Donahue16, Fu17}.
\item Unsupervised \cite{Jaderberg16} or self-supervised predictive \cite{Shelhamer17} auxiliary tasks as proxies supporting state representation learning. In the same arena, predicting the future as a proxy to learning to act \cite{Dosovitskiy16, Oh17}.
\item Integrating state representation learning on multi-task \cite{Kulkarni16, Andreas17, Wilson07,Teh17}, multi-agent \cite{Foerster16,Foerster17}, hierarchical \cite{Wilson07} and model-agnostic meta-learning \cite{Finn17Model} settings, e.g., involving human-robot value alignment as in cooperative inverse reinforcement learning (CIRL, a different notion of inverse RL \cite{Hadfield16}) and applications beyond robotics (see \cite{Li17,Kaelbling96} for other domains).
\item Full integrations of action, perception and learning into a single process (e.g., as done in \cite{LeGoff17} to produce a saliency map) shall be integrated with state representation learning  approaches to blend into the RL pipeline.

\item Reward engineering. The needs for handcrafting reward functions in RL is a current challenge impeding transfer learning. Automatic reward acquisition in high-dimensional settings with unknown dynamics is hard in practice, and approaches such as AIRL (Adversarial Inverse RL) are robust first steps \cite{Fu17}.

\item Data sampling efficiency and simulation to real RL: A strategy that is helping speed up state representation learning is using more data sample efficient approaches such as demonstration learning \cite{Hester17}.

%\item integrating causality predictive models into state representation learning?  %Works using causality without learning state spaces are \cite{Lopez-Paz16,Rojas-Carulla17}. These are predictive vision algorithms that are showing the power of learning causal rules directly from large amounts of raw unlabeled data such as images. Interestingly, exploiting these kind of causal signals could improve state representation learning in future work. %\tim{does Lopez-Paz16 and Rojas-Carulla17 use the prediction to learn a state?NAT: No, they are predictive approaches of pixel-level features, as insight of a different approach without learning states.}

\end{itemize}


In the area to tackle the "reality gap", i.e., the challenge of training in simulation for testing in reality, recent approaches are beating the state of the art, for instance \cite{Peng17}.%  https://arxiv.org/pdf/1704.03732.pdf %   Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap." By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics,



Some of the latest approaches to bridge this gap use successful techniques such as domain randomization \cite{Tobin17} and adaptation \cite{Bousmalis17} but also, on top of that, dynamics randomization \cite{Peng17}, which allows learning in real life with only data from (differently calibrated and sensor dynamics) simulators. In an object-pushing task, the authors demonstrate that training exclusively in simulation, allows to learn policies that are robust to calibration errors while maintaining performance levels.

Unfortunately, many of the successful approaches on transferring simulation to real life control and RL use methods that require motion capture devices for setting and tracking the goal when either the goal itself changes, or when the target object's relative position varies in between episodes. Often, these systems draw on motion capture systems such as \textit{Optitrack} (for e.g. teaching Baxter robot a highly diverse repertoire of tasks including throwing a ball into a basket \cite{Kim17}) and \textit{PhaseSpace} mocap system \cite{Peng17} (to determine that the goal is considered satisfied if the puck being pushed is within a certain distance of the target).

State representation learning could help to cross the reality gap for RL by using high level representation.

\subsection{Promising Deep Reinforcement Learning Approaches for state representation learning}

Research directions worth exploring include neuroscience-inspired AI \cite{Hassabis17} approaches as well as hybrid neural-symbolic \cite{Garnelo16} approaches, where learning from scratch as is a common recent norm is not the trend any more, but starting with a pre-wired basic relationships, can allow better online-learning and notions such as \textit{compositionality} in concept learning. One interesting approach in this area is the generative vision model which can be highlighted for his high data efficiency that is able to break text-based CAPTCHAs \cite{George17}. % from nice blog https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/

Likewise, adversarial approaches to feature learning must be explored for state representation learning \cite{Donahue16}, as well as how to integrate options when multi-goal settings are present. Promising directions are in \cite{Machado17a}, where they implicitly learn and discover options (acting at different time scales) through using Proto-value functions (PVFs, an approach for representation learning in MDPs) and \textit{eigenpurposes}, intrinsic reward functions derived from the learned representations. %tested for tabular domains (what are these exactly?) and Atari 2600 games (deterministic vs stochastic?).

\section{Validation Methods, Frameworks and Benchmarks}
This section discusses application contexts, metrics and datasets used in the quality evaluation of states learned. 

% Other more general ways of assessing CNN's performance is measuring networks receptive field TODO: \cite{}online Tool], or attention maps. Other methods are described below.
% - **On the Quantitative Evaluation of Deep Generative Models** <br> *Russ Salakhutdinov*  [pdf](www.cs.cmu.edu/~rsalakhu/talk_Eval.pdf)
% - **A new embedding quality assessment method for manifold learning** <br> *Zhang11*. [pdf](https://arxiv.org/pdf/1108.1636v1.pdf)


\subsection{Interpretability  and evaluation of learned representations}
When it comes too give a quantitative value which estimates the quality of a representation, in the context of representation learning this can be harder than expected. The assessment should show if the representation we learned is conform to what we expect. While toolboxes to benchmark the robustness of machine learning models exists \cite{Rauber17}, understanding intermediate layers during the development of a learning methods can be challenging. An approach in this direction is using linear classifier probes \cite{Alain16}, as well as using class-enhanced attentive responses \cite{Kumar17}. The latter enables the visualization of the activations for the most dominant classes.  Other way of assessing dimensionality reduction algorithms is in terms of their loss of quality \cite{Gracia14}, while adversarial methods can be tested with a suite of adversarial attacks \cite{Rauber17}. For instance, Foolbox\footnote{\url{https://pypi.python.org/pypi/foolbox}} creates adversarial examples that fool neural networks with gradient-based and blackbox attacks. % TODO and can we cite a paper and not a slidedeck from On the Quantitative Evaluation of Deep Generative Models Russ Salakhutdinov*  [pdf](www.cs.cmu.edu/~rsalakhu/talk_Eval.pdf)?

%Linear classifier probe: We start from the concept of Shanon entropy, which is the classic way to describe the information contents of a random variable. We then seek to apply that concept to understand the roles of the intermediate layers of a neural network, to measure how much information is gained at every layer (answer : technically, none). We show that it fails to apply, and so we propose an alternative framework to ask the same question again.

The deep reinforcement learning (DRL) community has made available a large set of software benchmarks to test RL algorithms \cite{Arulkumaran17}. 
Despite the growing interest in making robotic applications work in real life settings, simulation environments are the de-facto start point and where most of the algorithms design, learning and evolution happens. 
Within the robotics domain, most used middleware is open-source: ROS, YARP, OROCOS, Player, etc. and some, also cross-platform \cite{Ivaldi14}. The top 7 most currently used simulators, in decreasing order, are Gazebo, ODE, Bullet, OpenRave, V-Rep, XDE, and Blender. Examples of most recently added RL Gym environments to PyBullet\footnote{\url{http://pybullet.org}} Minitaur quadruped, MIT racecar, KUKA grasping, Ant, Hopper, Humanoid etc. 
These simulation environments are mainly designed for RL, but they could equally be used to test state representation algorithms.

While this choice allows roboticists to pick the best middleware for their needs, it also makes comparing AI models very challenging, since dynamics models and simulators' performance comparison easily introduces noise, calibration issues and instabilities that are susceptible of making the algorithms sensible to them.

Despite the above being the best softwares upon user ratings \cite{Ivaldi14}, the challenge of not having reproducible datasets for robotics both in simulation and real live makes these simulators practically unusable as platforms to support research benchmarks. This is why most research builds on RL toy tasks or the more broad set of Atari 2600 video games from the Arcade Learning Environment (ALE) \cite{Bellemare13,Machado17}. However, the further development of similar standard datasets and benchmarks for robotic control tasks is a crucial gap yet to filled.

Examples of challenging games that involve further planning and strategic thinking include Freeway and Montezuma's revenge. \textit{Freeway} is a game in which a chicken is expected to cross the road while avoiding cars, well suited to observe options in which the agent clearly wants to reach a specific lane in the street, %a histogram representing the chicken’s height during an episode is used for evaluation We can clearly see how the chicken’s height varies for different
since a random walk over primitive actions does not explore the environment properly as shown in \cite{Machado17a}. %Remarkably, option #445 scores 28 points at the end of the episode, without ever explicitly taking the reward signal into consideration. This performance is very close to those obtained by state-of-the-art algorithms.
In Montezuma's revenge, the objective is to navigate a room to pickup a key to open a door, and thus, option discovery and valuation is key (see \cite{Kulkarni16} and \cite{Machado17a}).



\subsection{Validation metrics}
This section provides a listing on metrics and embedding quality evaluation techniques used across the literature. These are summarized by table \ref{tab:metrics}.

\begin{table}[!htbp] 
\caption{State representations quality evaluation criteria}
\label{tab:metrics}
%\vskip 0.15in
\begin{center}
\begin{small}
%\begin{tabular}{lll}
\begin{tabular}{p{45mm}|p{45mm}|p{45mm}}
\textbf{Metric} &
\textbf{ Evaluation target} &
\textbf{Context/Observations}\\\hline\hline

Disentanglement metric score \cite{Higgins16} &
Data-generating latent factor disentanglement  &
Transfer learning    \\\hline

% Inception score \cite{Zhao16} and extensions \cite{Gurumurthy17} &
% Generative models (GAN) &
% Correlates well with human evaluation \\\hline

Distortion \cite{Indyk01} &
Preservation of local and global geometry coherence  & 
Unsupervised Representation learning    \\\hline

% Frechet Inception Distance (FID)\cite{Heusel17}  &
% GANs  &
% Improved successor of inception \\\hline

NIEQA \cite{Zhang12} & 
Normalization Independent Embedding Quality Assessment &
Manifold learning \\\hline 

%& & \\\hline
\end{tabular}
\end{small}
\end{center}
\end{table}

Visual assessment of the representation's quality can be done using a Nearest-Neighbors approach as in \cite{Sermanet17} for example, or in \cite{Pinto16}.   The idea is to look at the Nearest Neighbors in the learned representation space, and for each neighbor, retrieve their corresponding image and associated ground truth state-representative dimension we are interested in measuring. 

Specially when learning state representations with priors, since we want to impose local coherence (especially the temporal prior), a good representation should have local coherence, and therefore, the associated ground truth states should be close. While the nearest neighbor coherence can be assessed visually, KNN-MSE is a quantitative metric from this information\cite{Lesort17}. Using the ground truth value for every image, it measures the distance between the value of the original image and the value on the nearest neighbor images retrieved in the learned state space\footnote{A low distance means that a neighbor in the ground truth is still a neighbor in the learned representation, and thus, local coherence is conserved}.

For an image $I$, KNN-MSE is computed as follows: 
\begin{equation}\label{eq:knn_mse_crit}
\textrm{KNN-MSE}(I)=\frac{1}{k}\sum_{I' \in KNN(I,k) } || \phi(I) - \phi(I') ||^2
\end{equation}
where $\textrm{KNN}(I,k)$ returns the $k$ nearest neighbors of $I$ in the learned state space and $\phi(I)$ gives the ground truth ($s$) associated to I. 

Other metrics comparable to KNN-MSE are distortion \cite{Indyk01} and NIEQA \cite{Zhang11}; both share the same principle as two quantitative measures of the global quality of a representation: the representation space should, as much as possible, be an undistorted version of the original space.%, meaning circles (and ellipses) in the original space stay as circles (and ellipses) in the representation space. %The space doesn't have to be on the same scale (ex: if 2 lines of 5cm and 10cm respectively are transformed into lines of 2mm and 4mm, it doesn't mean that the representation space is bad, because only the scale changed). 
%Moreover, the goal is to have a globally coherent space (and not only locally, as measured by KNN-MSE). Here we define different version of the distortion and NIEQA to assess the quality of representation.
%$S$ correspond to the ground truth space, $S_i$ is an element of it, $\hat{S}$ is the representation space and $\hat{S}_i$ an element of it.
%\begin{equation}  
%Global\_Distortion(S,\hat{S}) = \frac{\max_{i\neq j} d(i,j) }{\min_{i\neq j} d(i,j)}
%\end{equation}
%\begin{equation}
%\mbox{ where } d(i,j)= \frac{||S_i, S_j||^2}{||\hat{S}_i,\hat{S}_j||^2}
%\end{equation}
%From this version, we derived 2 other distortion measures called Local distortion and Far distortion. The idea behind local distortion is to compute the distortion only with close pairs of points, i.e., instead of taking every $i,j$ pair, we only compute distortion when $||Si,Sj||^2 < \epsilon$. $\epsilon$ depends on the dataset. The \textit{far distortion} is just the opposite, we only consider the distortion for points whose distance is greater than $\epsilon$: $||Si,Sj||^2  > \epsilon$

NIEQA (Normalization Independent Embedding Quality Assessment) \cite{Zhang12} is a more complex evaluation that measures the local geometry quality and the global topology quality of a representation. NIEQA local part checks if the representation is locally equivalent to an Euclidean subspace that preserves the structure of local neighborhoods. NIEQA objectives are aligned with KNN-MSE \cite{Lesort17}, as a measure to assess the quality of the representation, especially locally. The global NIEQA measure is also based on the idea of preserving original structure in the representation space, but instead of looking at the neighbors, it samples ”representative” points in the whole state space. Then, it considers the preservation of the geodesic distance between those points in the state space.

Regarding the assessment of next observation's predictions, complementary feature learning strategies beyond MSE include multi-scale architectures, adversarial training methods, and image gradient difference loss functions as proposed in \cite{Mathieu15}. More concretely, the \textit{Peak Signal to Noise Ratio, Structural Similarity Index Measure} and \textit{image sharpness} show to be the proxies for next frame prediction assessment \cite{Mathieu15}.  Architectures and losses may be used as building blocks for more sophisticated prediction models, involving memory and recurrence. Because unlike most optical flow algorithms, the model is fully differentiable, it can be fine-tuned for task transfer learning. \natalia{I havent added these  to the table because they evaluate next observation, not the state representation formally, not sure if makes sense to add any}

\subsection{Datasets and benchmarks}

Datasets used to validate state representation learning include varied, but mainly simulated environments. Unlike in image recognition challenges where MNIST digits or ImageNet dataset prevail, in state representation learning, a varied set of regular video games or visuomotor tasks in robotics can be found as a test suite for robotics control.

We list in this subsection a compilation of benchmarks used in the cited papers, from classic RL test beds to the latest most challenging Atari games. Evaluation benchmarks found evaluated in mentioned papers in this survey include Mario Bros, octopus game, labyrinths, mazes, driving cars, or controlled open source simulated RL environments such as the inverted and regular pendulum, car in mountain scenario, or the bouncing ball. Many of the latter are part of the Universe and OpenAI Gym\footnote{OpenAI Gym environment classic control problems in RL: \url{https://gym.openai.com/envs}}\cite{Brockman16} or DeepMind Labs \cite{Beattie16}. Few of them involve a 3D input space; e.g. VizDoom (as used in the Intrinsic Curiosity Module of \cite{Pathak17}) or in the robotics domain, Baxter robot performing tasks such as pushing a button \cite{Lesort17} or poking objects \cite{Agrawal16}.

Some of the most challenging games being tackled at the moment are those that require ahead thinking in order to learn states, actions and a more longer term planning. These include the road-crossing chicken game \cite{Machado17a} or Montezuma's Revenge Atari game, which happens to be the one with the worse performance when it comes to human-level control by AI agents \cite{Mnih15}. A comprehensive benchmark on DRL for continuous control is in \cite{Duan16}, including locomotion and hierarchical tasks as main testbeds: Hopper, 2D Walker, Half-Cheetah, Simple/Full Humanoid, Cart-Pole Balancing, Inverted Pendulum, Mountain Car, Acrobot, Swimmer + Gathering/Maze task, and Ant + Gathering task. The benchmarked algorithms in continuous control include batch algorithms such as Truncated Natural Policy Gradient (TNPG), Reward-Weighted Regression (RWR), Trust Region Policy Optimization (TRPO), Cross Entropy Method (CEM), Relative Entropy Policy Search (REPS) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), as well as on-line algorithms such as Deep Deterministic Policy Gradient (DDPG). These are all mainly  deep end-to-end approaches; however, learning an intermediate state representation could help boosting performance.

Benchmarking tasks used in the most prominent state representation learning literature is summarized in Table \ref{tab:benchmarks}.



\begin{table}[!htbp]%\begin{sidewaystable}[!htbp]
\centering
\caption{Benchmark and Tasks Used to evaluate State Representation Learning Algorithms}
\label{tab:benchmarks}
\begin{tabular}{p{49mm}|p{55mm}|p{55mm}}
\textbf{Task/Benchmark} & \textbf{ Models using it} &\textbf{Observations}\\\hline\hline
1)Pushing and (Baxter) poking objects and 2)Robotic pushing dataset  & 1)\cite{Peng17,Agrawal16}, 2) SV2P \cite{Babaeizadeh17}, video pixel networks (VPN) \cite{Kalchbrenner16,Reed17} & Motor babbling be used for unsupervised learning of object saliency maps \cite{LeGoff17}
\\\hline
Robotic grasping & Using domain adaptation and simulation \cite{Bousmalis17} & \\\hline
Cart-pole  & \cite{Watter15}, PVE \cite{Jonschkowski17}  & \\\hline 
Ball in cup &  \cite{Kim17},PVE \cite{Jonschkowski17}  & \\\hline
Inverted pendulum  & \cite{Watter15}, PVE \cite{Jonschkowski17}  &  \\\hline
Dynamic Pendulum  & Deep Variational Bayes Filters Unsupervised learning of state space models from raw data \cite{Karl16}  &   \\\hline
Driving car 2D  & \cite{Jonschkowski15} &  \\\hline
Shopping Cart Task  & Physics based  model prior \cite{Scholz14} & task is to push a shopping-cart to the goal  \\\hline
Apartment Rearrangement Task  & Physics based  model prior\cite{Scholz14} & task is a multi-object rearrangement problem in a simulated  apartment
%Simulation Box2D engine \cite{Catto13}
\\\hline
Control of a three-link arm  & \cite{Watter15}  & \\\hline
The 2-link arm problem  & \cite{Munk16} & \\\hline
The octopus problem  & \cite{Munk16} & \\\hline
A real-robot manipulation task based on tactile feedback  & \cite{Hoof16} & \\\hline
A PR2 robot on tasks including pushing a free-standing toy block & \cite{Finn15} & picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions     \\\hline
Bouncing ball   & \cite{Karl16}   & \\\hline
NORB dataset of generated simulated movies & \cite{Goroshin15} & \\\hline
simulated high-dimensional, vision-based mobile robot planning task  & \cite{Boots09}   &  \\\hline
Super Mario Bros \& VizDoom & \cite{Pathak17} & 2 and 3D resp.\\\hline
Baxter pushing button RL with Image benchmark for robotic priors  & \cite{Lesort17}   & 3D \\\hline
Montezuma's revenge & \cite{Machado17a} & \\\hline
Freeway (chicken crossing road) & \cite{Machado17a} & \\\hline
Tetherball game (robot has to hit a ball hanging from a pole without giving the opponent the chance
to unwind it)  & Used in reward-weighted PCA (rwPCA) goal-driven algorithm \cite{Parisi17} & \\\hline
(finite) Taxi domain and (continuous) Linear Quadratic Gaussian Regulator and Car on the Hill environments & Reward Compatible IRL \cite{Metelli17} & \\\hline
Atari games and (determ. and stochastic -reward position change-) Collect Domain & VPN \cite{Oh17} blending learning and planning using lookahead for discrete control problems  & \textit{Collect} Objective: to pick as much rewards as possible in minimal time. \\\hline
\end{tabular}
\end{table}%\end{sidewaystable}
%\end{landscape} % this keeps table as regular but intercalates a landscape sheet in between (rotates the paper type, not the table!)

Because choosing the state dimensionality or number of features a priori can be challenging, a promising direction avoiding this step is using return-based feature construction without relying on the value function to select the optimal action (at the cost of fixing the explained variance) \cite{Parisi17}.


\section{Discussion}

This survey discussed different points of view and approaches to tackle the learning of state representations in unsupervised or semi-supervised manners. The following table (Table \ref{tab:papers}) summarizes the main particularities of each main work, including objective functions and mechanisms the current state of the art uses in order to achieve this goal. When the learning of the states is not done directly, we indicate if a proxy element is predicted in order to aid the learning of the state representations. We can see that the vast majority of approaches focus on end-to-end approaches for very specific tasks, although a varied set of works in the literature are nowadays blending different of these approaches at once to improve performance, the quality of the representations, their interpretability, or most importantly, their transferability to other tasks. Table \ref{tab:papers} schematically summarizes the main works discussed with the hope of helping provide more contextual grounding and intuition to the different approaches, as well as intermediate proxies predicted, or used in the task of state representation learning.


\begin{sidewaystable}[!htbp]
\centering
\caption{Approaches with proxy tasks used for state representation learning: \textit{Proxy}, \textit{Reward} and \textit{Time} columns indicate if the approach uses rewards (\textit{Rw}), uses time-based observations (\textit{Time}), or a third element proxy (\textit{Proxy}) in order to learn the state (versus learning the state directly such as e.g., VAEs).} 
\label{tab:papers}
%\begin{tabular}{llllllll}
\begin{tabular}{p{42mm}|p{20mm}|p{20mm}|p{13mm}|p{10mm}|p{10mm}|p{30mm}|p{20mm}}
\textbf{Model and Ref.} & \textbf{Supervised} & \textbf{Objective funct.} & \textbf{Uses Reward} & \textbf{Uses Proxy} & \textbf{Time}  & \textbf{Observations} & \textbf{Requires prior knowledge} \\\hline\hline

VAE \cite{Kingma13} & No & Variational free energy & no  & 0 & 0 & yes  & \\\hline
InfoGAN \cite{Chen16}   & No & Mutual information & no  & 0  & 0 & scalable & yes \\\hline
DC-IGN \cite{Kulkarni15}  & Semi &  & no      & 0  & 0   & yes & \\\hline
betaVAE \cite{Higgins16}   & Semi & Variational free energy \footnote{\cite{Jordan99} if b=1} & no & 0  & 0 & stable   & No  \\\hline
PVE \cite{Jonschkowski17} & No & & no & 0 & 0 &  & yes  \\\hline      
Loss as reward \cite{Shelhamer17}  & No & Auxiliary task loss & No & Aux. tasks & Yes & Augments policy gradient with aux. gradients from self-supervised tasks & No \\\hline 
\end{tabular}
\end{sidewaystable}


In similar ways as priors about the physical world can help learning state representations \cite{Jonschkowski15,Lesort17}, and learning policies guided by sketches and curriculum learning improves multi-task learning performance with shared policies for discrete and continuous control and sparse rewards \cite{Andreas17}, integrating broader state representation learning strategies into RL policy learning is a latent topic of interesting research.



\section{Acknowledgements}
This research is funded by the DREAM project under the European Union's Horizon 2020 research and innovation program under grant agreement No 640891.

\bibliographystyle{apalike}
\bibliography{references}

\end{document}